<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · NeuralAttentionlib.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://chengchingwen.github.io/NeuralAttentionlib.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">NeuralAttentionlib.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../term/">Terminology</a></li><li><a class="tocitem" href="../example/">Example</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Functional"><span>Functional</span></a></li><li><a class="tocitem" href="#Mask"><span>Mask</span></a></li><li><a class="tocitem" href="#Matmul"><span>Matmul</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/master/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><ul><li><a href="#Base.:!-Tuple{NeuralAttentionlib.AbstractMask}"><code>Base.:!</code></a></li><li><a href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:&amp;</code></a></li><li><a href="#Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:|</code></a></li><li><a href="#NeuralAttentionlib.AttenMask"><code>NeuralAttentionlib.AttenMask</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.attention_score"><code>NeuralAttentionlib.attention_score</code></a></li><li><a href="#NeuralAttentionlib.biased_score"><code>NeuralAttentionlib.biased_score</code></a></li><li><a href="#NeuralAttentionlib.collapsed_size"><code>NeuralAttentionlib.collapsed_size</code></a></li><li><a href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>NeuralAttentionlib.collapseddims</code></a></li><li><a href="#NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}"><code>NeuralAttentionlib.collapseddims</code></a></li><li><a href="#NeuralAttentionlib.dot_product_score"><code>NeuralAttentionlib.dot_product_score</code></a></li><li><a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>NeuralAttentionlib.generic_multihead_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.generic_qkv_attention"><code>NeuralAttentionlib.generic_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.get_sincos_position_embeddings"><code>NeuralAttentionlib.get_sincos_position_embeddings</code></a></li><li><a href="#NeuralAttentionlib.getmask"><code>NeuralAttentionlib.getmask</code></a></li><li><a href="#NeuralAttentionlib.layer_norm"><code>NeuralAttentionlib.layer_norm</code></a></li><li><a href="#NeuralAttentionlib.masked_score"><code>NeuralAttentionlib.masked_score</code></a></li><li><a href="#NeuralAttentionlib.matmul"><code>NeuralAttentionlib.matmul</code></a></li><li><a href="#NeuralAttentionlib.merge_head"><code>NeuralAttentionlib.merge_head</code></a></li><li><a href="#NeuralAttentionlib.mixing"><code>NeuralAttentionlib.mixing</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_in"><code>NeuralAttentionlib.move_head_dim_in</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_in_perm"><code>NeuralAttentionlib.move_head_dim_in_perm</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_out"><code>NeuralAttentionlib.move_head_dim_out</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_out_perm"><code>NeuralAttentionlib.move_head_dim_out_perm</code></a></li><li><a href="#NeuralAttentionlib.multihead_qkv_attention"><code>NeuralAttentionlib.multihead_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.naive_qkv_attention"><code>NeuralAttentionlib.naive_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.noncollapsed_size"><code>NeuralAttentionlib.noncollapsed_size</code></a></li><li><a href="#NeuralAttentionlib.normalized_score"><code>NeuralAttentionlib.normalized_score</code></a></li><li><a href="#NeuralAttentionlib.rms_layer_norm"><code>NeuralAttentionlib.rms_layer_norm</code></a></li><li><a href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>NeuralAttentionlib.scalar_relative_position_embedding</code></a></li><li><a href="#NeuralAttentionlib.scaled_dot_product_score"><code>NeuralAttentionlib.scaled_dot_product_score</code></a></li><li><a href="#NeuralAttentionlib.scaled_matmul"><code>NeuralAttentionlib.scaled_matmul</code></a></li><li><a href="#NeuralAttentionlib.split_head"><code>NeuralAttentionlib.split_head</code></a></li><li><a href="#NeuralAttentionlib.t5_bucketed_position_id"><code>NeuralAttentionlib.t5_bucketed_position_id</code></a></li><li><a href="#NeuralAttentionlib.t5_causal_bucketed_position_id"><code>NeuralAttentionlib.t5_causal_bucketed_position_id</code></a></li><li><a href="#NeuralAttentionlib.unwrap_collapse"><code>NeuralAttentionlib.unwrap_collapse</code></a></li><li><a href="#NeuralAttentionlib.weighted_sum_mixing"><code>NeuralAttentionlib.weighted_sum_mixing</code></a></li><li><a href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractMask"><code>NeuralAttentionlib.AbstractMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractMaskOp"><code>NeuralAttentionlib.AbstractMaskOp</code></a></li><li><a href="#NeuralAttentionlib.AbstractSequenceMask"><code>NeuralAttentionlib.AbstractSequenceMask</code></a></li><li><a href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a></li><li><a href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a></li><li><a href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a></li><li><a href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a></li><li><a href="#NeuralAttentionlib.CollapsedDimsArray"><code>NeuralAttentionlib.CollapsedDimsArray</code></a></li><li><a href="#NeuralAttentionlib.GenericAttenMask"><code>NeuralAttentionlib.GenericAttenMask</code></a></li><li><a href="#NeuralAttentionlib.LengthMask"><code>NeuralAttentionlib.LengthMask</code></a></li><li><a href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a></li><li><a href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a></li><li><a href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a></li><li><a href="#NeuralAttentionlib.RevBiLengthMask"><code>NeuralAttentionlib.RevBiLengthMask</code></a></li><li><a href="#NeuralAttentionlib.RevLengthMask"><code>NeuralAttentionlib.RevLengthMask</code></a></li><li><a href="#NeuralAttentionlib.RevSymLengthMask"><code>NeuralAttentionlib.RevSymLengthMask</code></a></li><li><a href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a></li></ul><h2 id="Functional"><a class="docs-heading-anchor" href="#Functional">Functional</a><a id="Functional-1"></a><a class="docs-heading-anchor-permalink" href="#Functional" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.attention_score" href="#NeuralAttentionlib.attention_score"><code>NeuralAttentionlib.attention_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">attention_score(f, args...) = f(args...)</code></pre><p>Attention score api. Can be overload for doing custom implementation with <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>.  <code>f</code> is the score function.</p><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>, <a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>generic_multihead_qkv_attention</code></a>, <a href="#NeuralAttentionlib.mixing"><code>mixing</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L89-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.biased_score" href="#NeuralAttentionlib.biased_score"><code>NeuralAttentionlib.biased_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">biased_score(bias, score, args...)</code></pre><p>Adding a precomputed <code>bias</code> to the attention score. <code>bias</code> should be in shape <code>(key length, query length, ...)</code> and  <code>size(bias, 1) == size(s, 1) == size(bias, 2) == size(s, 2) &amp;&amp; ndims(bias) &lt;= ndims(s)</code> where <code>s = score(args...)</code>  must hold.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L142-L148">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.dot_product_score" href="#NeuralAttentionlib.dot_product_score"><code>NeuralAttentionlib.dot_product_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dot_product_score(q, k)</code></pre><p>Dot-product attention score function. Equivalent to <code>scaled_dot_product_score(q, k, 1)</code>.</p><p>See also: <a href="#NeuralAttentionlib.scaled_dot_product_score"><code>scaled_dot_product_score</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L133-L139">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.generic_multihead_qkv_attention" href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>NeuralAttentionlib.generic_multihead_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generic_multihead_qkv_attention(mixingf, scoref, head, q, k, v, args...)</code></pre><p>Generic version of <a href="#NeuralAttentionlib.multihead_qkv_attention"><code>multihead_qkv_attention</code></a>. Need to specify mixing and score function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L28-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.generic_qkv_attention" href="#NeuralAttentionlib.generic_qkv_attention"><code>NeuralAttentionlib.generic_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generic_qkv_attention(mixingf, scoref, q, k, v, args...)</code></pre><p>Generic version of <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>. Need to specify mixing and score function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L21-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.get_sincos_position_embeddings" href="#NeuralAttentionlib.get_sincos_position_embeddings"><code>NeuralAttentionlib.get_sincos_position_embeddings</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_sincos_position_embeddings(hidden_size::Integer, normalized::Bool, x)</code></pre><p>sincos position embeddings. <code>x</code> can be either a integer specifying the length or an array of position indices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L184-L188">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.layer_norm" href="#NeuralAttentionlib.layer_norm"><code>NeuralAttentionlib.layer_norm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">layer_norm([epsilon = 1e-5,] alpha, beta, x)</code></pre><p>Function which perform layer normalization on <code>x</code>. <code>alpha</code> and <code>beta</code> can a <code>Vector</code>, <code>Number</code> or <code>Nothing</code>.</p><p><span>$layer_norm(α, β, x) = α\frac{(x - μ)}{σ} + β$</span></p><p>If both <code>alpha</code> and <code>beta</code> is <code>Nothing</code>, this is just a standardize function applied on the first dimension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L269-L277">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.masked_score" href="#NeuralAttentionlib.masked_score"><code>NeuralAttentionlib.masked_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">masked_score(mask) = masked_score $ mask
masked_score(maskop, mask) = masked_score $ maskop $ mask
masked_score(maskop::AbstractMaskOp, mask::AbstractMask, score, args...)</code></pre><p>Masked attention score api. Applying the <code>mask</code> according to <code>maskop</code> on the attention score  compute from <code>score(args...)</code>.</p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>, <a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L110-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.merge_head" href="#NeuralAttentionlib.merge_head"><code>NeuralAttentionlib.merge_head</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">merge_head(x)</code></pre><p>merge the <code>head</code> dimension split by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L262-L266">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.mixing" href="#NeuralAttentionlib.mixing"><code>NeuralAttentionlib.mixing</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mixing(f, v, g, args...) = f(attention_score(g, args...), v)</code></pre><p><code>Mixing</code> function api. Can be overload for doing custom implementation with <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>.  <code>f</code> is the mixing function and <code>g</code> is score function.</p><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>, <a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>generic_multihead_qkv_attention</code></a>, <a href="#NeuralAttentionlib.attention_score"><code>attention_score</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L72-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_in" href="#NeuralAttentionlib.move_head_dim_in"><code>NeuralAttentionlib.move_head_dim_in</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_in(x::AbstractArray, nobatch=false)</code></pre><p>Equivanlent to <code>permutedims(x, move_head_dim_in_perm(x, nobatch)))</code></p><p>See also: <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_in_perm"><code>move_head_dim_in_perm</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L253-L259">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_in_perm" href="#NeuralAttentionlib.move_head_dim_in_perm"><code>NeuralAttentionlib.move_head_dim_in_perm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_in_perm(x::AbstractArray{T, N}, nobatch=false)
move_head_dim_in_perm(N::Int, nobatch=false)</code></pre><p>Dimension order for <code>permutedims</code> to move the <code>head</code> dimension (created by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>) from batch dimension  to feature dimension (for <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>). Return a tuple of integer of length <code>n</code>.  <code>nobatch</code> specify where <code>x</code> is a batch of data.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; Functional.move_head_dim_in_perm(5, false)
(1, 4, 2, 3, 5)

julia&gt; Functional.move_head_dim_in_perm(5, true)
(1, 5, 2, 3, 4)
</code></pre><p>See also: <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_in"><code>move_head_dim_in</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L230-L250">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_out" href="#NeuralAttentionlib.move_head_dim_out"><code>NeuralAttentionlib.move_head_dim_out</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_out(x::AbstractArray, nobatch=false)</code></pre><p>Equivanlent to <code>permutedims(x, move_head_dim_out_perm(x, nobatch)))</code></p><p>See also: <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_out_perm"><code>move_head_dim_out_perm</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L221-L227">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_out_perm" href="#NeuralAttentionlib.move_head_dim_out_perm"><code>NeuralAttentionlib.move_head_dim_out_perm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_out_perm(x::AbstractArray{T, N}, nobatch=false)
move_head_dim_out_perm(N::Int, nobatch=false)</code></pre><p>Dimension order for <code>permutedims</code> to move the <code>head</code> dimension (created by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>) to batch dimension.  Return a tuple of integer of length <code>n</code>. <code>nobatch</code> specify where <code>x</code> is a batch of data.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; Functional.move_head_dim_out_perm(5, false)
(1, 3, 4, 2, 5)

julia&gt; Functional.move_head_dim_out_perm(5, true)
(1, 3, 4, 5, 2)
</code></pre><p>See also: <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_out"><code>move_head_dim_out</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L199-L218">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.multihead_qkv_attention" href="#NeuralAttentionlib.multihead_qkv_attention"><code>NeuralAttentionlib.multihead_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">multihead_qkv_attention(head, q, k, v, mask=nothing)</code></pre><p>Multihead version of <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>. The core operation for implement a regular transformer layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L35-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.naive_qkv_attention" href="#NeuralAttentionlib.naive_qkv_attention"><code>NeuralAttentionlib.naive_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">naive_qkv_attention(q, k, v, mask=nothing)</code></pre><p>The scaled dot-product attention of a regular transformer layer.</p><p><span>$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</span></p><p>It&#39;s equivalent to <code>generic_qkv_attention(weighted_sum_mixing, normalized_score(NNlib.softmax) $ masked_score(GenericMaskOp(), mask) $ scaled_dot_product_score, q, k, v)</code>.</p><p>#Example</p><pre><code class="language-julia hljs">julia&gt; fdim, ldim, bdim = 32, 10, 4;

julia&gt; x = randn(fdim, ldim, bdim);

julia&gt; y = naive_qkv_attention(x, x, x); # simple self attention

# no mask here
julia&gt; z = generic_qkv_attention(weighted_sum_mixing, normalized_score(NNlib.softmax) $ scaled_dot_product_score, x, x, x);

julia&gt; y ≈ z
true
</code></pre><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L42-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.normalized_score" href="#NeuralAttentionlib.normalized_score"><code>NeuralAttentionlib.normalized_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">normalized_score(norm) = normalized_score $ norm
normalized_score(norm, score, args...)</code></pre><p>Normalized attenion score api. <code>norm</code> is the normalize function (like <code>softmax</code>) and <code>score</code> is the function  that compute attention score from <code>args...</code>.</p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L99-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.rms_layer_norm" href="#NeuralAttentionlib.rms_layer_norm"><code>NeuralAttentionlib.rms_layer_norm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rms_layer_norm([epsilon = 1e-5,] alpha, x)</code></pre><p>Function which perform root-mean-square layer normalization on <code>x</code>. <code>alpha</code> and <code>beta</code> can a <code>Vector</code>, <code>Number</code>  or <code>Nothing</code>.</p><p><span>$rms_layer_norm(α, x) = α\frac{x}{\sqrt{\sum_{i=1}^{N} x^2 / N}}$</span></p><p>If both <code>alpha</code> is <code>Nothing</code>, this is just a normalization with root-mean-square function applied on the first  dimension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L280-L290">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.scalar_relative_position_embedding" href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>NeuralAttentionlib.scalar_relative_position_embedding</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">scalar_relative_position_embedding(relative_position_id_func, embedding_table, score, args...)</code></pre><p>A relative position embedding that produce a trainable scalar bias for each value in the attention score.  <code>relative_position_id_func</code> is a function that take the attention score and return a <code>relative_position_id</code>  matrix with the same size of the attention score with batches (normally <code>(key length, query length)</code>). This  <code>relative_position_id</code> would be used to index (or <code>gather</code>) the <code>embedding_table</code>. <code>embedding_table</code> is an  array with multiple dimensions, where the first dimension is the number of possible <code>&quot;id&quot;</code>s and the remaining  dimensions are for giving different value to each heads. By default we treat the last dimension of attention  score as the batch dimension and the dimension between last dimension and the &quot;length&quot; dimension as the head  dimensions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L151-L162">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.scaled_dot_product_score" href="#NeuralAttentionlib.scaled_dot_product_score"><code>NeuralAttentionlib.scaled_dot_product_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> scaled_dot_product_score(q, k, s = sqrt(inv(size(k, 1))))</code></pre><p>The scaled dot-product attention score function of a regular transformer layer.</p><p><span>$Score(Q, K) = \frac{QK^T}{\sqrt{d_k}}$</span></p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L122-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.split_head" href="#NeuralAttentionlib.split_head"><code>NeuralAttentionlib.split_head</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">split_head(head::Int, x)</code></pre><p>Split the first dimension into <code>head</code> piece of small vector. Equivalent to  <code>reshape(x, :, head, tail(size(x))...)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L191-L196">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.t5_bucketed_position_id" href="#NeuralAttentionlib.t5_bucketed_position_id"><code>NeuralAttentionlib.t5_bucketed_position_id</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">t5_bucketed_position_id(n_buckets::Int, max_distance::Int)</code></pre><p>A <code>relative_position_id_func</code> used in the T5 Transformer model. The relative distances is assigned to a  logarithmical buecket and the distance beyond <code>max_distance</code> would be assigned to the same bucket.</p><p>See also: <a href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>scalar_relative_position_embedding</code></a>, <a href="#NeuralAttentionlib.t5_causal_bucketed_position_id"><code>t5_causal_bucketed_position_id</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L165-L172">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.t5_causal_bucketed_position_id" href="#NeuralAttentionlib.t5_causal_bucketed_position_id"><code>NeuralAttentionlib.t5_causal_bucketed_position_id</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">t5_causal_bucketed_position_id(n_buckets::Int, max_distance::Int)</code></pre><p>Same as <code>t5_bucketed_position_id</code> but only attent to past. Should be used with <a href="#NeuralAttentionlib.CausalMask"><code>CausalMask</code></a></p><p>See also: <a href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>scalar_relative_position_embedding</code></a>, <a href="#NeuralAttentionlib.t5_bucketed_position_id"><code>t5_bucketed_position_id</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L175-L181">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.weighted_sum_mixing" href="#NeuralAttentionlib.weighted_sum_mixing"><code>NeuralAttentionlib.weighted_sum_mixing</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">weighted_sum_mixing(s, v)</code></pre><p>The mixing function of a regular transformer layer. <code>s</code> is the attention score and <code>v</code> is the value of QKV attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/functional.jl#L82-L86">source</a></section></article><h2 id="Mask"><a class="docs-heading-anchor" href="#Mask">Mask</a><a id="Mask-1"></a><a class="docs-heading-anchor-permalink" href="#Mask" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractAttenMask" href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractAttenMask &lt;: AbstractMask</code></pre><p>Abstract type for mask data specifically for attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/mask/mask.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractMask" href="#NeuralAttentionlib.AbstractMask"><code>NeuralAttentionlib.AbstractMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractMask</code></pre><p>Abstract type for mask data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/mask/mask.jl#L8-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractMaskOp" href="#NeuralAttentionlib.AbstractMaskOp"><code>NeuralAttentionlib.AbstractMaskOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractMaskOp</code></pre><p>Trait-like abstract type for holding operation related argument, defined how the mask should be apply to input array</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/mask/mask.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractSequenceMask" href="#NeuralAttentionlib.AbstractSequenceMask"><code>NeuralAttentionlib.AbstractSequenceMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractSequenceMask &lt;: AbstractMask</code></pre><p>Abstract type for mask data specifically for sequence.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/mask/mask.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::GenericMaskOp, mask::AbstractMask, score)</code></pre><p>Equivalent to <code>op.apply(score, op.scale .* (op.flip ? .! mask : mask))</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(10, 10);

julia&gt; m = CausalMask()
CausalMask()

julia&gt; apply_mask(GenericMaskOp(.+, true, -1e9), m, x) ==  @. x + (!m * -1e9)
true
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/mask/mask.jl#L65-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::NaiveMaskOp, mask::AbstractMask, score)</code></pre><p>Directly broadcast multiply mask to attention score, i.e. <code>score .* mask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/mask/mask.jl#L37-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractArrayMask" href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractArrayMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask with array data</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractDatalessMask" href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractDatalessMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask without array data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L16-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BandPartMask" href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BandPartMask(l::Int, u::Int) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that only allow <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/band_part">band_part</a>  values to pass.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L65-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BatchedMask" href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BatchedMask(mask::AbstractMask) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for applying the same mask within the same batch.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 2, 2) .* m
ERROR: DimensionMismatch(&quot;arrays could not be broadcast to a common size; mask require ndims(A) == 3&quot;)
Stacktrace:
[...]

julia&gt; trues(3,3, 2, 2) .* BatchedMask(m) # 4-th dim become batch dim
3×3×2×2 BitArray{4}:
[:, :, 1, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 1, 2] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 2, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L293-L344">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BiLengthMask" href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BiLengthMask(q_len::A, k_len::A) where {A &lt;: AbstractArray{Int, N}} &lt;: AbstractArrayMask</code></pre><p>Attention mask specified by two arrays of integer that indicate the length dimension size.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; bm = BiLengthMask([2,3], [3, 5])
BiLengthMask{1, Vector{Int32}}(Int32[2, 3], Int32[3, 5])

julia&gt; trues(5,5, 2) .* bm
5×5×2 BitArray{3}:
[:, :, 1] =
 1  1  0  0  0
 1  1  0  0  0
 1  1  0  0  0
 0  0  0  0  0
 0  0  0  0  0

[:, :, 2] =
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
</code></pre><p>See also: <a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L110-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.CausalMask" href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CausalMask() &lt;: AbstractDatalessMask</code></pre><p>Attention mask that block the future values.</p><p>Similar to applying <code>LinearAlgebra.triu!</code> on the score matrix</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L37-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.GenericAttenMask" href="#NeuralAttentionlib.GenericAttenMask"><code>NeuralAttentionlib.GenericAttenMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GenericAttenMask &lt;: AbstractArrayMask</code></pre><p>Generic attention mask. Just a wrapper over <code>AbstractArray{Bool}</code> for dispatch.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L73-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.LengthMask" href="#NeuralAttentionlib.LengthMask"><code>NeuralAttentionlib.LengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LengthMask(len::AbstractArray{Int, N}) &lt;: AbstractSequenceMask</code></pre><p>A Sequence Mask specified by an array of integer that indicate the length dimension size.  Can be convert to attention mask (<a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>) with <a href="#NeuralAttentionlib.AttenMask"><code>AttenMask</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; ones(7, 7, 2) .* LengthMask([3, 5])
7×7×2 Array{Float64, 3}:
[:, :, 1] =
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0

[:, :, 2] =
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L143-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.LocalMask" href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LocalMask(width::Int) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that only allow local (diagonal like) values to pass.</p><p><code>width</code> should be ≥ 0 and <code>A .* LocalMask(1)</code> is similar to <code>Diagonal(A)</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L46-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RandomMask" href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomMask(p::Float64) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that block value randomly.</p><p><code>p</code> specify the percentage of value to block. e.g. <code>A .* RandomMask(0)</code> is equivalent to <code>identity(A)</code> and  <code>A .* RandomMask(1)</code> is equivalent to <code>zero(A)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L55-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RepeatMask" href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RepeatMask(mask::AbstractMask, num::Int) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for doing inner repeat on the last dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 4) .* m
ERROR: DimensionMismatch(&quot;arrays could not be broadcast to a common size; mask require 3-th dimension to be 2, but get 4&quot;)
Stacktrace:
[...]

julia&gt; trues(3,3, 4) .* RepeatMask(m, 2)
3×3×4 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 3] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 4] =
 1  1  1
 1  1  1
 1  1  1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L347-L398">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RevBiLengthMask" href="#NeuralAttentionlib.RevBiLengthMask"><code>NeuralAttentionlib.RevBiLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RevBiLengthMask(q_len::A, k_len::A) where {A &lt;: AbstractArray{Int, N}} &lt;: AbstractArrayMask</code></pre><p><a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a> but counts from the end of array, used for left padding.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; bm = RevBiLengthMask([2,3], [3, 5])
RevBiLengthMask{1, Vector{Int32}}(Int32[2, 3], Int32[3, 5])

julia&gt; trues(5,5, 2) .* bm
5×5×2 BitArray{3}:
[:, :, 1] =
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  1  1
 0  0  0  1  1
 0  0  0  1  1

[:, :, 2] =
 0  0  1  1  1
 0  0  1  1  1
 0  0  1  1  1
 0  0  1  1  1
 0  0  1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.RevLengthMask"><code>RevLengthMask</code></a>, <a href="#NeuralAttentionlib.RevSymLengthMask"><code>RevSymLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L205-L235">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RevLengthMask" href="#NeuralAttentionlib.RevLengthMask"><code>NeuralAttentionlib.RevLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RevLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractSequenceMask</code></pre><p><a href="#NeuralAttentionlib.LengthMask"><code>LengthMask</code></a> but counts from the end of array, used for left padding.  Can be convert to attention mask (<a href="#NeuralAttentionlib.RevSymLengthMask"><code>RevSymLengthMask</code></a>, <a href="#NeuralAttentionlib.RevBiLengthMask"><code>RevBiLengthMask</code></a>) with <a href="#NeuralAttentionlib.AttenMask"><code>AttenMask</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; ones(7, 7, 2) .* RevLengthMask([3, 5])
7×7×2 Array{Float64, 3}:
[:, :, 1] =
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0

[:, :, 2] =
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L238-L268">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RevSymLengthMask" href="#NeuralAttentionlib.RevSymLengthMask"><code>NeuralAttentionlib.RevSymLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RevSymLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractArrayMask</code></pre><p><a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a> but counts from the end of array, used for left padding.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = RevSymLengthMask([2,3])
RevSymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 0  0  0
 0  1  1
 0  1  1

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L176-L202">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.SymLengthMask" href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractArrayMask</code></pre><p>Attention mask specified by an array of integer that indicate the length dimension size.  assuming <em>Query</em> length and <em>Key</em> length are the same.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.LengthMask"><code>LengthMask</code></a>, <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L80-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:!-Tuple{NeuralAttentionlib.AbstractMask}" href="#Base.:!-Tuple{NeuralAttentionlib.AbstractMask}"><code>Base.:!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">!m::AbstractMask</code></pre><p>Boolean not of an attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L271-L275">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}" href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:&amp;</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractMask &amp; m2::AbstractMask</code></pre><p>logical and of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L286-L290">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}" href="#Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:|</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractMask | m2::AbstractMask</code></pre><p>logical or of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L278-L282">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AttenMask" href="#NeuralAttentionlib.AttenMask"><code>NeuralAttentionlib.AttenMask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AttenMask(m::AbstractMask)</code></pre><p>convert mask into corresponding attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L30-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.getmask" href="#NeuralAttentionlib.getmask"><code>NeuralAttentionlib.getmask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">getmask(m::AbstractMask, score, scale = 1)</code></pre><p>Convert <code>m</code> into mask array of <code>AbstractArray</code> for <code>score</code> with <code>scale</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; getmask(CausalMask(), randn(7,7), 2)
7×7 Matrix{Float64}:
 2.0  2.0  2.0  2.0  2.0  2.0  2.0
 0.0  2.0  2.0  2.0  2.0  2.0  2.0
 0.0  0.0  2.0  2.0  2.0  2.0  2.0
 0.0  0.0  0.0  2.0  2.0  2.0  2.0
 0.0  0.0  0.0  0.0  2.0  2.0  2.0
 0.0  0.0  0.0  0.0  0.0  2.0  2.0
 0.0  0.0  0.0  0.0  0.0  0.0  2.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/mask.jl#L401-L419">source</a></section></article><h2 id="Matmul"><a class="docs-heading-anchor" href="#Matmul">Matmul</a><a id="Matmul-1"></a><a class="docs-heading-anchor-permalink" href="#Matmul" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.CollapsedDimsArray" href="#NeuralAttentionlib.CollapsedDimsArray"><code>NeuralAttentionlib.CollapsedDimsArray</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CollapsedDimsArray{T}(array, ni::Integer, nj::Integer) &lt;: AbstractArray{T, 3}</code></pre><p>Similar to lazy reshape array with <a href="#NeuralAttentionlib.collapsed_size"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L9-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.collapsed_size" href="#NeuralAttentionlib.collapsed_size"><code>NeuralAttentionlib.collapsed_size</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">collapsed_size(x, ni, nj [, n])::Dim{3}</code></pre><p>Collapse the dimensionality of <code>x</code> into 3 according to <code>ni</code> and <code>nj</code> where <code>ni</code>, <code>nj</code> specify the number of  second and third dimensions it take.</p><pre><code class="nohighlight hljs">(X1, X2, ..., Xk, Xk+1, Xk+2, ..., Xk+ni, Xk+ni+1, ..., Xn)
 |______dim1___|  |_________ni_________|  |______nj______|</code></pre><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(7,6,5,4,3,2);

julia&gt; collapsed_size(x, 2, 2, 1)
42

julia&gt; collapsed_size(x, 2, 2, 2)
20

julia&gt; collapsed_size(x, 2, 2, 3)
6

julia&gt; collapsed_size(x, 2, 2)
(42, 20, 6)
</code></pre><p>See also: <a href="#NeuralAttentionlib.noncollapsed_size"><code>noncollapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L49-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}" href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>NeuralAttentionlib.collapseddims</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">collapseddims(x::AbstractArray, xi, xj)</code></pre><p>Reshape <code>x</code> into 3 dim array, equivalent to <code>reshape(x, collapsed_size(x, xi, xj))</code></p><p>See also: <a href="#NeuralAttentionlib.collapsed_size"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L81-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}" href="#NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}"><code>NeuralAttentionlib.collapseddims</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">collapseddims(ca::CollapsedDimsArray)</code></pre><p>remove the wrapper and really reshape it.</p><p>See also: <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a>, <a href="#NeuralAttentionlib.unwrap_collapse"><code>unwrap_collapse</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L90-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.matmul" href="#NeuralAttentionlib.matmul"><code>NeuralAttentionlib.matmul</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">matmul(a::AbstractArray, b::AbstractArray, s::Number = 1)</code></pre><p>Equivalent to <code>s .* (a * b)</code> if <code>a</code> and <code>b</code> are <code>Vector</code> or <code>Matrix</code>. For array with higher dimension,  it will convert <code>a</code> and <code>b</code> to <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a> and perform batched matrix multiplication, and then  return the result as <code>CollapsedDimsArray</code>. This is useful for preserving the dimensionality. If the batch dimension  of <code>a</code> and <code>b</code> have different shape, it pick the shape of <code>b</code> for batch dimension. Work with <code>NNlib.batch_transpose</code>  and <code>NNlib.batch_adjoint</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># b-dim shape: (6,)
julia&gt; a = CollapsedDimsArray(randn(3,4,2,3,6), 2, 1); size(a)
(12, 6, 6)

# b-dim shape: (3,1,2)
julia&gt; b = CollapsedDimsArray(randn(6,2,3,1,2), 1, 3); size(b)
(6, 2, 6)

julia&gt; c = matmul(a, b); size(c), typeof(c)
((12, 2, 6), CollapsedDimsArray{Float64, Array{Float64, 6}, Static.StaticInt{1}, Static.StaticInt{3}})

# b-dim shape: (3,1,2)
julia&gt; d = unwrap_collapse(c); size(d), typeof(d)
((3, 4, 2, 3, 1, 2), Array{Float64, 6})

# equivanlent to `batched_mul` but preserve shape
julia&gt; NNlib.batched_mul(collapseddims(a), collapseddims(b)) == collapseddims(matmul(a, b))
true
</code></pre><p>See also: <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a>, <a href="#NeuralAttentionlib.unwrap_collapse"><code>unwrap_collapse</code></a>, <a href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>collapseddims</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L106-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.noncollapsed_size" href="#NeuralAttentionlib.noncollapsed_size"><code>NeuralAttentionlib.noncollapsed_size</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">noncollapsed_size(x, ni, nj [, n])</code></pre><p>Collapse the dimensionality of <code>x</code> into 3 according to <code>ni</code> and <code>nj</code>.</p><pre><code class="nohighlight hljs">(X1, X2, ..., Xk, Xk+1, Xk+2, ..., Xk+ni, Xk+ni+1, ..., Xn)
 |______dim1___|  |_________ni_________|  |______nj______|</code></pre><p>But take the size before collapse. e.g. <code>noncollapsed_size(x, ni, nj, 2)</code> will be <code>(Xi, Xi+1, ..., Xj-1)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(7,6,5,4,3,2);

julia&gt; noncollapsed_size(x, 2, 2, 1)
(7, 6)

julia&gt; noncollapsed_size(x, 2, 2, 2)
(5, 4)

julia&gt; noncollapsed_size(x, 2, 2, 3)
(3, 2)

julia&gt; noncollapsed_size(x, 2, 2)
((7, 6), (5, 4), (3, 2))
</code></pre><p>See also: <a href="#NeuralAttentionlib.collapsed_size"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L16-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.scaled_matmul" href="#NeuralAttentionlib.scaled_matmul"><code>NeuralAttentionlib.scaled_matmul</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">scaled_matmul(a::AbstractArray, b::AbstractArray, s::Number = 1)</code></pre><p>Basically equivalent to <code>unwrap_collapse(matmul(a, b, s))</code>, but not differentiable w.r.t. to <code>s</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L143-L147">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.unwrap_collapse" href="#NeuralAttentionlib.unwrap_collapse"><code>NeuralAttentionlib.unwrap_collapse</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">unwrap_collapse(ca::CollapsedDimsArray)</code></pre><p>Return the underlying array of <code>CollapsedDimsArray</code>, otherwise just return the input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/37ba356fb01adea435693b3335d418ed9e8d176c/src/module/matmul.jl#L99-L103">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example/">« Example</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Sunday 18 December 2022 17:32">Sunday 18 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
