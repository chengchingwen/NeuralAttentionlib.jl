<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · NeuralAttentionlib.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://chengchingwen.github.io/NeuralAttentionlib.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">NeuralAttentionlib.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../term/">Terminology</a></li><li><a class="tocitem" href="../example/">Example</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Functional"><span>Functional</span></a></li><li><a class="tocitem" href="#Mask"><span>Mask</span></a></li><li><a class="tocitem" href="#Matmul"><span>Matmul</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/master/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><ul><li><a href="#Base.:!-Tuple{NeuralAttentionlib.AbstractMask}"><code>Base.:!</code></a></li><li><a href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:&amp;</code></a></li><li><a href="#Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:|</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.attention_score"><code>NeuralAttentionlib.attention_score</code></a></li><li><a href="#NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}"><code>NeuralAttentionlib.collapseddims</code></a></li><li><a href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>NeuralAttentionlib.collapseddims</code></a></li><li><a href="#NeuralAttentionlib.dot_product_score"><code>NeuralAttentionlib.dot_product_score</code></a></li><li><a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>NeuralAttentionlib.generic_multihead_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.generic_qkv_attention"><code>NeuralAttentionlib.generic_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.getmask"><code>NeuralAttentionlib.getmask</code></a></li><li><a href="#NeuralAttentionlib.masked_score"><code>NeuralAttentionlib.masked_score</code></a></li><li><a href="#NeuralAttentionlib.matmul"><code>NeuralAttentionlib.matmul</code></a></li><li><a href="#NeuralAttentionlib.merge_head"><code>NeuralAttentionlib.merge_head</code></a></li><li><a href="#NeuralAttentionlib.mixing"><code>NeuralAttentionlib.mixing</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_in"><code>NeuralAttentionlib.move_head_dim_in</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_in_perm"><code>NeuralAttentionlib.move_head_dim_in_perm</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_out"><code>NeuralAttentionlib.move_head_dim_out</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_out_perm"><code>NeuralAttentionlib.move_head_dim_out_perm</code></a></li><li><a href="#NeuralAttentionlib.multihead_qkv_attention"><code>NeuralAttentionlib.multihead_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.naive_qkv_attention"><code>NeuralAttentionlib.naive_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.noncollapsed_size"><code>NeuralAttentionlib.noncollapsed_size</code></a></li><li><a href="#NeuralAttentionlib.normalized_score"><code>NeuralAttentionlib.normalized_score</code></a></li><li><a href="#NeuralAttentionlib.scaled_dot_product_score"><code>NeuralAttentionlib.scaled_dot_product_score</code></a></li><li><a href="#NeuralAttentionlib.split_head"><code>NeuralAttentionlib.split_head</code></a></li><li><a href="#NeuralAttentionlib.unwrap_collapse"><code>NeuralAttentionlib.unwrap_collapse</code></a></li><li><a href="#NeuralAttentionlib.weighted_sum_mixing"><code>NeuralAttentionlib.weighted_sum_mixing</code></a></li><li><a href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractMask"><code>NeuralAttentionlib.AbstractMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractMaskOp"><code>NeuralAttentionlib.AbstractMaskOp</code></a></li><li><a href="#NeuralAttentionlib.AbstractSequenceMask"><code>NeuralAttentionlib.AbstractSequenceMask</code></a></li><li><a href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a></li><li><a href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a></li><li><a href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a></li><li><a href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a></li><li><a href="#NeuralAttentionlib.CollapsedDimsArray"><code>NeuralAttentionlib.CollapsedDimsArray</code></a></li><li><a href="#NeuralAttentionlib.GenericAttenMask"><code>NeuralAttentionlib.GenericAttenMask</code></a></li><li><a href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a></li><li><a href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a></li><li><a href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a></li><li><a href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a></li></ul><h2 id="Functional"><a class="docs-heading-anchor" href="#Functional">Functional</a><a id="Functional-1"></a><a class="docs-heading-anchor-permalink" href="#Functional" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.attention_score" href="#NeuralAttentionlib.attention_score"><code>NeuralAttentionlib.attention_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">attention_score(f, args...) = f(args...)</code></pre><p>Attention score api. Can be overload for doing custom implementation with <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>.  <code>f</code> is the score function.</p><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>, <a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>generic_multihead_qkv_attention</code></a>, <a href="#NeuralAttentionlib.mixing"><code>mixing</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L86-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.dot_product_score" href="#NeuralAttentionlib.dot_product_score"><code>NeuralAttentionlib.dot_product_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dot_product_score(q, k)</code></pre><p>Dot-product attention score function. Equivalent to <code>scaled_dot_product_score(q, k, 1)</code>.</p><p>See also: <a href="#NeuralAttentionlib.scaled_dot_product_score"><code>scaled_dot_product_score</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L130-L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.generic_multihead_qkv_attention" href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>NeuralAttentionlib.generic_multihead_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generic_multihead_qkv_attention(mixingf, scoref, head, q, k, v, args...)</code></pre><p>Generic version of <a href="#NeuralAttentionlib.multihead_qkv_attention"><code>multihead_qkv_attention</code></a>. Need to specify mixing and score function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L25-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.generic_qkv_attention" href="#NeuralAttentionlib.generic_qkv_attention"><code>NeuralAttentionlib.generic_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generic_qkv_attention(mixingf, scoref, q, k, v, args...)</code></pre><p>Generic version of <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>. Need to specify mixing and score function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L18-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.masked_score" href="#NeuralAttentionlib.masked_score"><code>NeuralAttentionlib.masked_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">masked_score(mask) = masked_score $ mask
masked_score(maskop, mask) = masked_score $ maskop $ mask
masked_score(maskop::AbstractMaskOp, mask::AbstractMask, score, args...)</code></pre><p>Masked attention score api. Applying the <code>mask</code> according to <code>maskop</code> on the attention score  compute from <code>score(args...)</code>.</p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>, <a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L107-L116">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.merge_head" href="#NeuralAttentionlib.merge_head"><code>NeuralAttentionlib.merge_head</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">merge_head(x)</code></pre><p>merge the <code>head</code> dimension split by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L210-L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.mixing" href="#NeuralAttentionlib.mixing"><code>NeuralAttentionlib.mixing</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mixing(f, v, g, args...) = f(attention_score(g, args...), v)</code></pre><p><code>Mixing</code> function api. Can be overload for doing custom implementation with <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>.  <code>f</code> is the mixing function and <code>g</code> is score function.</p><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>, <a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>generic_multihead_qkv_attention</code></a>, <a href="#NeuralAttentionlib.attention_score"><code>attention_score</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L69-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_in" href="#NeuralAttentionlib.move_head_dim_in"><code>NeuralAttentionlib.move_head_dim_in</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_in(x::AbstractArray, nobatch=false)</code></pre><p>Equivanlent to <code>permutedims(x, move_head_dim_in_perm(x, nobatch)))</code></p><p>See also: <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_in_perm"><code>move_head_dim_in_perm</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L201-L207">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_in_perm" href="#NeuralAttentionlib.move_head_dim_in_perm"><code>NeuralAttentionlib.move_head_dim_in_perm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_in_perm(x::AbstractArray{T, N}, nobatch=false)
move_head_dim_in_perm(N::Int, nobatch=false)</code></pre><p>Dimension order for <code>permutedims</code> to move the <code>head</code> dimension (created by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>) from batch dimension  to feature dimension (for <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>). Return a tuple of integer of length <code>n</code>.  <code>nobatch</code> specify where <code>x</code> is a batch of data.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; Functional.move_head_dim_in_perm(5, false)
(1, 4, 2, 3, 5)

julia&gt; Functional.move_head_dim_in_perm(5, true)
(1, 5, 2, 3, 4)
</code></pre><p>See also: <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_in"><code>move_head_dim_in</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L178-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_out" href="#NeuralAttentionlib.move_head_dim_out"><code>NeuralAttentionlib.move_head_dim_out</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_out(x::AbstractArray, nobatch=false)</code></pre><p>Equivanlent to <code>permutedims(x, move_head_dim_out_perm(x, nobatch)))</code></p><p>See also: <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_out_perm"><code>move_head_dim_out_perm</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L169-L175">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_out_perm" href="#NeuralAttentionlib.move_head_dim_out_perm"><code>NeuralAttentionlib.move_head_dim_out_perm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_out_perm(x::AbstractArray{T, N}, nobatch=false)
move_head_dim_out_perm(N::Int, nobatch=false)</code></pre><p>Dimension order for <code>permutedims</code> to move the <code>head</code> dimension (created by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>) to batch dimension.  Return a tuple of integer of length <code>n</code>. <code>nobatch</code> specify where <code>x</code> is a batch of data.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; Functional.move_head_dim_out_perm(5, false)
(1, 3, 4, 2, 5)

julia&gt; Functional.move_head_dim_out_perm(5, true)
(1, 3, 4, 5, 2)
</code></pre><p>See also: <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_out"><code>move_head_dim_out</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L147-L166">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.multihead_qkv_attention" href="#NeuralAttentionlib.multihead_qkv_attention"><code>NeuralAttentionlib.multihead_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">multihead_qkv_attention(head, q, k, v, mask=nothing)</code></pre><p>Multihead version of <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>. The core operation for implement a regular transformer layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L32-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.naive_qkv_attention" href="#NeuralAttentionlib.naive_qkv_attention"><code>NeuralAttentionlib.naive_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">naive_qkv_attention(q, k, v, mask=nothing)</code></pre><p>The scaled dot-product attention of a regular transformer layer.</p><p><span>$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</span></p><p>It&#39;s equivalent to <code>generic_qkv_attention(weighted_sum_mixing, normalized_score(NNlib.softmax) $ masked_score(GenericMaskOp(), mask) $ scaled_dot_product_score, q, k, v)</code>.</p><p>#Example</p><pre><code class="language-julia hljs">julia&gt; fdim, ldim, bdim = 32, 10, 4;

julia&gt; x = randn(fdim, ldim, bdim);

julia&gt; y = naive_qkv_attention(x, x, x); # simple self attention

# no mask here
julia&gt; z = generic_qkv_attention(weighted_sum_mixing, normalized_score(NNlib.softmax) $ scaled_dot_product_score, x, x, x);

julia&gt; y ≈ z
true
</code></pre><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L39-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.normalized_score" href="#NeuralAttentionlib.normalized_score"><code>NeuralAttentionlib.normalized_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">normalized_score(norm) = normalized_score $ norm
normalized_score(norm, score, args...)</code></pre><p>Normalized attenion score api. <code>norm</code> is the normalize function (like <code>softmax</code>) and <code>score</code> is the function  that compute attention score from <code>args...</code>.</p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L96-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.scaled_dot_product_score" href="#NeuralAttentionlib.scaled_dot_product_score"><code>NeuralAttentionlib.scaled_dot_product_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> scaled_dot_product_score(q, k, s = sqrt(inv(size(k, 1))))</code></pre><p>The scaled dot-product attention score function of a regular transformer layer.</p><p><span>$Score(Q, K) = \frac{QK^T}{\sqrt{d_k}}$</span></p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L119-L127">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.split_head" href="#NeuralAttentionlib.split_head"><code>NeuralAttentionlib.split_head</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">split_head(head::Int, x)</code></pre><p>Split the first dimension into <code>head</code> piece of small vector. Equivalent to  <code>reshape(x, :, head, tail(size(x))...)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L139-L144">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.weighted_sum_mixing" href="#NeuralAttentionlib.weighted_sum_mixing"><code>NeuralAttentionlib.weighted_sum_mixing</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">weighted_sum_mixing(s, v)</code></pre><p>The mixing function of a regular transformer layer. <code>s</code> is the attention score and <code>v</code> is the value of QKV attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/functional.jl#L79-L83">source</a></section></article><h2 id="Mask"><a class="docs-heading-anchor" href="#Mask">Mask</a><a id="Mask-1"></a><a class="docs-heading-anchor-permalink" href="#Mask" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractAttenMask" href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractAttenMask &lt;: AbstractMask</code></pre><p>Abstract type for mask data specifically for attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/mask/mask.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractMask" href="#NeuralAttentionlib.AbstractMask"><code>NeuralAttentionlib.AbstractMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractMask</code></pre><p>Abstract type for mask data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/mask/mask.jl#L8-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractMaskOp" href="#NeuralAttentionlib.AbstractMaskOp"><code>NeuralAttentionlib.AbstractMaskOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractMaskOp</code></pre><p>Trait-like abstract type for holding operation related argument, defined how the mask should be apply to input array</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/mask/mask.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractSequenceMask" href="#NeuralAttentionlib.AbstractSequenceMask"><code>NeuralAttentionlib.AbstractSequenceMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractSequenceMask &lt;: AbstractMask</code></pre><p>Abstract type for mask data specifically for sequence.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/mask/mask.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::GenericMaskOp, mask::AbstractMask, score)</code></pre><p>Equivalent to <code>op.apply(score, op.scale .* (op.flip ? .! mask : mask))</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(10, 10);

julia&gt; m = CausalMask()
CausalMask()

julia&gt; apply_mask(GenericMaskOp(.+, true, -1e9), m, x) ==  @. x + (!m * -1e9)
true
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/mask/mask.jl#L68-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::NaiveMaskOp, mask::AbstractMask, score)</code></pre><p>Directly broadcast multiply mask to attention score, i.e. <code>score .* mask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/mask/mask.jl#L37-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractArrayMask" href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractArrayMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask with array data</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractDatalessMask" href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractDatalessMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask without array data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BandPartMask" href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BandPartMask(l::Int, u::Int) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that only allow <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/band_part">band_part</a>  values to pass.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L57-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BatchedMask" href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BatchedMask(mask::AbstractMask) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for applying the same mask within the same batch.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 2, 2) .* m
ERROR: DimensionMismatch(&quot;arrays could not be broadcast to a common size; mask require ndims(A) == 3&quot;)
Stacktrace:
[...]

julia&gt; trues(3,3, 2, 2) .* BatchedMask(m) # 4-th dim become batch dim
3×3×2×2 BitArray{4}:
[:, :, 1, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 1, 2] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 2, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L157-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BiLengthMask" href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BiLengthMask(q_len::A, k_len::A) where {A &lt;: AbstractArray{Int, N}} &lt;: AbstractArrayMask</code></pre><p>Attention mask specified by two arrays of integer that indicate the length dimension size.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; bm = BiLengthMask([2,3], [3, 5])
BiLengthMask{1, Vector{Int32}}(Int32[2, 3], Int32[3, 5])

julia&gt; trues(5,5, 2) .* bm
5×5×2 BitArray{3}:
[:, :, 1] =
 1  1  0  0  0
 1  1  0  0  0
 1  1  0  0  0
 0  0  0  0  0
 0  0  0  0  0

[:, :, 2] =
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
</code></pre><p>See also: <a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L102-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.CausalMask" href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CausalMask() &lt;: AbstractDatalessMask</code></pre><p>Attention mask that block the future values.</p><p>Similar to applying <code>LinearAlgebra.triu!</code> on the score matrix</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L29-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.GenericAttenMask" href="#NeuralAttentionlib.GenericAttenMask"><code>NeuralAttentionlib.GenericAttenMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GenericAttenMask &lt;: AbstractArrayMask</code></pre><p>Generic attention mask. Just a wrapper over <code>AbstractArray{Bool}</code> for dispatch.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L65-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.LocalMask" href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LocalMask(width::Int) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that only allow local (diagonal like) values to pass.</p><p><code>width</code> should be ≥ 0 and <code>A .* LocalMask(1)</code> is similar to <code>Diagonal(A)</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L38-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RandomMask" href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomMask(p::Float64) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that block value randomly.</p><p><code>p</code> specify the percentage of value to block. e.g. <code>A .* RandomMask(0)</code> is equivalent to <code>identity(A)</code> and  <code>A .* RandomMask(1)</code> is equivalent to <code>zero(A)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L47-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RepeatMask" href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RepeatMask(mask::AbstractMask, num::Int) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for doing inner repeat on the last dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 4) .* m
ERROR: DimensionMismatch(&quot;arrays could not be broadcast to a common size; mask require 3-th dimension to be 2, but get 4&quot;)
Stacktrace:
[...]

julia&gt; trues(3,3, 4) .* RepeatMask(m, 2)
3×3×4 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 3] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 4] =
 1  1  1
 1  1  1
 1  1  1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L211-L262">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.SymLengthMask" href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractArrayMask</code></pre><p>Attention mask specified by an array of integer that indicate the length dimension size.  assuming <em>Query</em> length and <em>Key</em> length are the same.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L72-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:!-Tuple{NeuralAttentionlib.AbstractMask}" href="#Base.:!-Tuple{NeuralAttentionlib.AbstractMask}"><code>Base.:!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">!m::AbstractMask</code></pre><p>Boolean not of an attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L135-L139">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}" href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:&amp;</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractMask &amp; m2::AbstractMask</code></pre><p>logical and of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L150-L154">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}" href="#Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:|</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractMask | m2::AbstractMask</code></pre><p>logical or of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L142-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.getmask" href="#NeuralAttentionlib.getmask"><code>NeuralAttentionlib.getmask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">getmask(m::AbstractMask, score, scale = 1)</code></pre><p>Convert <code>m</code> into mask array of <code>AbstractArray</code> for <code>score</code> with <code>scale</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; getmask(CausalMask(), randn(7,7), 2)
7×7 Matrix{Float64}:
 2.0  2.0  2.0  2.0  2.0  2.0  2.0
 0.0  2.0  2.0  2.0  2.0  2.0  2.0
 0.0  0.0  2.0  2.0  2.0  2.0  2.0
 0.0  0.0  0.0  2.0  2.0  2.0  2.0
 0.0  0.0  0.0  0.0  2.0  2.0  2.0
 0.0  0.0  0.0  0.0  0.0  2.0  2.0
 0.0  0.0  0.0  0.0  0.0  0.0  2.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/mask.jl#L265-L283">source</a></section></article><h2 id="Matmul"><a class="docs-heading-anchor" href="#Matmul">Matmul</a><a id="Matmul-1"></a><a class="docs-heading-anchor-permalink" href="#Matmul" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.CollapsedDimsArray" href="#NeuralAttentionlib.CollapsedDimsArray"><code>NeuralAttentionlib.CollapsedDimsArray</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CollapsedDimsArray{T}(array, ni::Integer, nj::Integer) &lt;: AbstractArray{T, 3}</code></pre><p>Similar to lazy reshape array with <a href="@ref"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/matmul.jl#L9-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}" href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>NeuralAttentionlib.collapseddims</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">collapseddims(x::AbstractArray, xi, xj)</code></pre><p>Reshape <code>x</code> into 3 dim array, equivalent to <code>reshape(x, collapsed_size(x, xi, xj))</code></p><p>See also: <a href="@ref"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/matmul.jl#L81-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}" href="#NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}"><code>NeuralAttentionlib.collapseddims</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">collapseddims(ca::CollapsedDimsArray)</code></pre><p>remove the wrapper and really reshape it.</p><p>See also: <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a>, <a href="#NeuralAttentionlib.unwrap_collapse"><code>unwrap_collapse</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/matmul.jl#L90-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.matmul" href="#NeuralAttentionlib.matmul"><code>NeuralAttentionlib.matmul</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">matmul(a::AbstractArray, b::AbstractArray, s::Number = 1)</code></pre><p>Equivalent to <code>s .* (a * b)</code> if <code>a</code> and <code>b</code> are <code>Vector</code> or <code>Matrix</code>. For array with higher dimension,  it will convert <code>a</code> and <code>b</code> to <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a> and perform batched matrix multiplication, and then  return the result as <code>CollapsedDimsArray</code>. This is useful for preserving the dimensionality. If the batch dimension  of <code>a</code> and <code>b</code> have different shape, it pick the shape of <code>b</code> for batch dimension. Work with <code>NNlib.batch_transpose</code>  and <code>NNlib.batch_adjoint</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># b-dim shape: (6,)
julia&gt; a = CollapsedDimsArray(randn(3,4,2,3,6), 2, 1); size(a)
(12, 6, 6)

# b-dim shape: (3,1,2)
julia&gt; b = CollapsedDimsArray(randn(6,2,3,1,2), 1, 3); size(b)
(6, 2, 6)

julia&gt; c = matmul(a, b); size(c), typeof(c)
((12, 2, 6), CollapsedDimsArray{Float64, Array{Float64, 6}, Static.StaticInt{1}, Static.StaticInt{3}})

# b-dim shape: (3,1,2)
julia&gt; d = unwrap_collapse(c); size(d), typeof(d)
((3, 4, 2, 3, 1, 2), Array{Float64, 6})

# equivanlent to `batched_mul` but preserve shape
julia&gt; NNlib.batched_mul(collapseddims(a), collapseddims(b)) == collapseddims(matmul(a, b))
true
</code></pre><p>See also: <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a>, <a href="#NeuralAttentionlib.unwrap_collapse"><code>unwrap_collapse</code></a>, <a href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>collapseddims</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/matmul.jl#L106-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.noncollapsed_size" href="#NeuralAttentionlib.noncollapsed_size"><code>NeuralAttentionlib.noncollapsed_size</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">noncollapsed_size(x, ni, nj [, n])</code></pre><p>Collapse the dimensionality of <code>x</code> into 3 according to <code>ni</code> and <code>nj</code>.</p><pre><code class="nohighlight hljs">(X1, X2, ..., Xk, Xk+1, Xk+2, ..., Xk+ni, Xk+ni+1, ..., Xn)
 |______dim1___|  |_________ni_________|  |______nj______|</code></pre><p>But take the size before collapse. e.g. <code>noncollapsed_size(x, ni, nj, 2)</code> will be <code>(Xi, Xi+1, ..., Xj-1)</code>.</p><p>#Example</p><pre><code class="language-julia hljs">julia&gt; x = randn(7,6,5,4,3,2);

julia&gt; noncollapsed_size(x, 2, 2, 1)
(7, 6)

julia&gt; noncollapsed_size(x, 2, 2, 2)
(5, 4)

julia&gt; noncollapsed_size(x, 2, 2, 3)
(3, 2)

julia&gt; noncollapsed_size(x, 2, 2)
((7, 6), (5, 4), (3, 2))
</code></pre><p>See also: <a href="@ref"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/matmul.jl#L16-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.unwrap_collapse" href="#NeuralAttentionlib.unwrap_collapse"><code>NeuralAttentionlib.unwrap_collapse</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">unwrap_collapse(ca::CollapsedDimsArray)</code></pre><p>Return the underlying array of <code>CollapsedDimsArray</code>, otherwise just return the input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/88158fe307d17e9f27bbdc85327a81137929e11d/src/module/matmul.jl#L99-L103">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example/">« Example</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Wednesday 27 July 2022 09:07">Wednesday 27 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
