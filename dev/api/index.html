<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · NeuralAttentionlib.jl</title><meta name="title" content="API Reference · NeuralAttentionlib.jl"/><meta property="og:title" content="API Reference · NeuralAttentionlib.jl"/><meta property="twitter:title" content="API Reference · NeuralAttentionlib.jl"/><meta name="description" content="Documentation for NeuralAttentionlib.jl."/><meta property="og:description" content="Documentation for NeuralAttentionlib.jl."/><meta property="twitter:description" content="Documentation for NeuralAttentionlib.jl."/><meta property="og:url" content="https://chengchingwen.github.io/NeuralAttentionlib.jl/api/"/><meta property="twitter:url" content="https://chengchingwen.github.io/NeuralAttentionlib.jl/api/"/><link rel="canonical" href="https://chengchingwen.github.io/NeuralAttentionlib.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">NeuralAttentionlib.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../term/">Terminology</a></li><li><a class="tocitem" href="../example/">Example</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Functional"><span>Functional</span></a></li><li><a class="tocitem" href="#Mask"><span>Mask</span></a></li><li><a class="tocitem" href="#Matmul"><span>Matmul</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/master/docs/src/api.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><ul><li><a href="#Base.:!-Tuple{NeuralAttentionlib.AbstractMask}"><code>Base.:!</code></a></li><li><a href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:&amp;</code></a></li><li><a href="#Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:|</code></a></li><li><a href="#NeuralAttentionlib.:\$-Tuple{Function, Any}"><code>NeuralAttentionlib.:$</code></a></li><li><a href="#NeuralAttentionlib.AttenMask"><code>NeuralAttentionlib.AttenMask</code></a></li><li><a href="#NeuralAttentionlib.GetIndexer"><code>NeuralAttentionlib.GetIndexer</code></a></li><li><a href="#NeuralAttentionlib.alibi_position_embedding"><code>NeuralAttentionlib.alibi_position_embedding</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.attention_score"><code>NeuralAttentionlib.attention_score</code></a></li><li><a href="#NeuralAttentionlib.biased_score"><code>NeuralAttentionlib.biased_score</code></a></li><li><a href="#NeuralAttentionlib.collapsed_size"><code>NeuralAttentionlib.collapsed_size</code></a></li><li><a href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>NeuralAttentionlib.collapseddims</code></a></li><li><a href="#NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}"><code>NeuralAttentionlib.collapseddims</code></a></li><li><a href="#NeuralAttentionlib.dot_product_score"><code>NeuralAttentionlib.dot_product_score</code></a></li><li><a href="#NeuralAttentionlib.generic_grouped_query_attention"><code>NeuralAttentionlib.generic_grouped_query_attention</code></a></li><li><a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>NeuralAttentionlib.generic_multihead_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.generic_qkv_attention"><code>NeuralAttentionlib.generic_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.get_sincos_position_embeddings"><code>NeuralAttentionlib.get_sincos_position_embeddings</code></a></li><li><a href="#NeuralAttentionlib.getmask"><code>NeuralAttentionlib.getmask</code></a></li><li><a href="#NeuralAttentionlib.grouped_query_attention"><code>NeuralAttentionlib.grouped_query_attention</code></a></li><li><a href="#NeuralAttentionlib.layer_norm"><code>NeuralAttentionlib.layer_norm</code></a></li><li><a href="#NeuralAttentionlib.lengths"><code>NeuralAttentionlib.lengths</code></a></li><li><a href="#NeuralAttentionlib.masked_score"><code>NeuralAttentionlib.masked_score</code></a></li><li><a href="#NeuralAttentionlib.matmul"><code>NeuralAttentionlib.matmul</code></a></li><li><a href="#NeuralAttentionlib.merge_head"><code>NeuralAttentionlib.merge_head</code></a></li><li><a href="#NeuralAttentionlib.mixing"><code>NeuralAttentionlib.mixing</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_in"><code>NeuralAttentionlib.move_head_dim_in</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_in_perm"><code>NeuralAttentionlib.move_head_dim_in_perm</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_out"><code>NeuralAttentionlib.move_head_dim_out</code></a></li><li><a href="#NeuralAttentionlib.move_head_dim_out_perm"><code>NeuralAttentionlib.move_head_dim_out_perm</code></a></li><li><a href="#NeuralAttentionlib.multihead_qkv_attention"><code>NeuralAttentionlib.multihead_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.naive_qkv_attention"><code>NeuralAttentionlib.naive_qkv_attention</code></a></li><li><a href="#NeuralAttentionlib.noncollapsed_size"><code>NeuralAttentionlib.noncollapsed_size</code></a></li><li><a href="#NeuralAttentionlib.normalized_score"><code>NeuralAttentionlib.normalized_score</code></a></li><li><a href="#NeuralAttentionlib.rms_layer_norm"><code>NeuralAttentionlib.rms_layer_norm</code></a></li><li><a href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>NeuralAttentionlib.scalar_relative_position_embedding</code></a></li><li><a href="#NeuralAttentionlib.scaled_dot_product_score"><code>NeuralAttentionlib.scaled_dot_product_score</code></a></li><li><a href="#NeuralAttentionlib.scaled_matmul"><code>NeuralAttentionlib.scaled_matmul</code></a></li><li><a href="#NeuralAttentionlib.split_head"><code>NeuralAttentionlib.split_head</code></a></li><li><a href="#NeuralAttentionlib.t5_bucketed_position_id"><code>NeuralAttentionlib.t5_bucketed_position_id</code></a></li><li><a href="#NeuralAttentionlib.t5_causal_bucketed_position_id"><code>NeuralAttentionlib.t5_causal_bucketed_position_id</code></a></li><li><a href="#NeuralAttentionlib.unwrap_collapse"><code>NeuralAttentionlib.unwrap_collapse</code></a></li><li><a href="#NeuralAttentionlib.weighted_sum_mixing"><code>NeuralAttentionlib.weighted_sum_mixing</code></a></li><li><a href="#NeuralAttentionlib.with_rotary_position_embedding"><code>NeuralAttentionlib.with_rotary_position_embedding</code></a></li><li><a href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractMask"><code>NeuralAttentionlib.AbstractMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractMaskOp"><code>NeuralAttentionlib.AbstractMaskOp</code></a></li><li><a href="#NeuralAttentionlib.AbstractSeqMask"><code>NeuralAttentionlib.AbstractSeqMask</code></a></li><li><a href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a></li><li><a href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a></li><li><a href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a></li><li><a href="#NeuralAttentionlib.BiSeqMask"><code>NeuralAttentionlib.BiSeqMask</code></a></li><li><a href="#NeuralAttentionlib.CausalGroupedQueryAttenOp"><code>NeuralAttentionlib.CausalGroupedQueryAttenOp</code></a></li><li><a href="#NeuralAttentionlib.CausalGroupedQueryAttenOpWithScore"><code>NeuralAttentionlib.CausalGroupedQueryAttenOpWithScore</code></a></li><li><a href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a></li><li><a href="#NeuralAttentionlib.CausalMultiheadQKVAttenOp"><code>NeuralAttentionlib.CausalMultiheadQKVAttenOp</code></a></li><li><a href="#NeuralAttentionlib.CausalMultiheadQKVAttenOpWithScore"><code>NeuralAttentionlib.CausalMultiheadQKVAttenOpWithScore</code></a></li><li><a href="#NeuralAttentionlib.CollapsedDimsArray"><code>NeuralAttentionlib.CollapsedDimsArray</code></a></li><li><a href="#NeuralAttentionlib.GenericAttenMask"><code>NeuralAttentionlib.GenericAttenMask</code></a></li><li><a href="#NeuralAttentionlib.GenericSeqMask"><code>NeuralAttentionlib.GenericSeqMask</code></a></li><li><a href="#NeuralAttentionlib.GroupedQueryAttenOp"><code>NeuralAttentionlib.GroupedQueryAttenOp</code></a></li><li><a href="#NeuralAttentionlib.GroupedQueryAttenOpWithScore"><code>NeuralAttentionlib.GroupedQueryAttenOpWithScore</code></a></li><li><a href="#NeuralAttentionlib.Indexer"><code>NeuralAttentionlib.Indexer</code></a></li><li><a href="#NeuralAttentionlib.LengthMask"><code>NeuralAttentionlib.LengthMask</code></a></li><li><a href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a></li><li><a href="#NeuralAttentionlib.MultiheadQKVAttenOp"><code>NeuralAttentionlib.MultiheadQKVAttenOp</code></a></li><li><a href="#NeuralAttentionlib.MultiheadQKVAttenOpWithScore"><code>NeuralAttentionlib.MultiheadQKVAttenOpWithScore</code></a></li><li><a href="#NeuralAttentionlib.NoMask"><code>NeuralAttentionlib.NoMask</code></a></li><li><a href="#NeuralAttentionlib.PrefixedFunction"><code>NeuralAttentionlib.PrefixedFunction</code></a></li><li><a href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a></li><li><a href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a></li><li><a href="#NeuralAttentionlib.RevBiLengthMask"><code>NeuralAttentionlib.RevBiLengthMask</code></a></li><li><a href="#NeuralAttentionlib.RevLengthMask"><code>NeuralAttentionlib.RevLengthMask</code></a></li><li><a href="#NeuralAttentionlib.RevSymLengthMask"><code>NeuralAttentionlib.RevSymLengthMask</code></a></li><li><a href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a></li></ul><h2 id="Functional"><a class="docs-heading-anchor" href="#Functional">Functional</a><a id="Functional-1"></a><a class="docs-heading-anchor-permalink" href="#Functional" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.alibi_position_embedding" href="#NeuralAttentionlib.alibi_position_embedding"><code>NeuralAttentionlib.alibi_position_embedding</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">alibi_position_embedding(mask::Union{AbstractAttenMask, Nothing}, score, args...)</code></pre><p>Add the non-trainable ALiBi position embedding to the attention score. The ALiBi embedding varied for each head, which  assuming the attention is multi-head variants. The first dimension of the batch dimension of the attention score is  treated as the head dimension (If used in single head attention, the alibi value would vary across batches).  <code>mask</code> can either be a attention mask or <code>nothing</code>. Usually, it is needed when there are gaps or prefix paddings  in the samples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L185-L193">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.attention_score" href="#NeuralAttentionlib.attention_score"><code>NeuralAttentionlib.attention_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">attention_score(f, args...) = f(args...)</code></pre><p>Attention score api. Can be overload for doing custom implementation with <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>.  <code>f</code> is the score function.</p><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>, <a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>generic_multihead_qkv_attention</code></a>, <a href="#NeuralAttentionlib.mixing"><code>mixing</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L105-L112">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.biased_score" href="#NeuralAttentionlib.biased_score"><code>NeuralAttentionlib.biased_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">biased_score(bias, score, args...)</code></pre><p>Adding a precomputed <code>bias</code> to the attention score. <code>bias</code> should be in shape <code>(key length, query length, ...)</code> and  <code>size(bias, 1) == size(s, 1) == size(bias, 2) == size(s, 2) &amp;&amp; ndims(bias) &lt;= ndims(s)</code> where <code>s = score(args...)</code>  must hold.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L162-L168">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.dot_product_score" href="#NeuralAttentionlib.dot_product_score"><code>NeuralAttentionlib.dot_product_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dot_product_score(q, k)</code></pre><p>Dot-product attention score function. Equivalent to <code>scaled_dot_product_score(q, k, 1)</code>.</p><p>See also: <a href="#NeuralAttentionlib.scaled_dot_product_score"><code>scaled_dot_product_score</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L153-L159">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.generic_grouped_query_attention" href="#NeuralAttentionlib.generic_grouped_query_attention"><code>NeuralAttentionlib.generic_grouped_query_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generic_grouped_query_attention(mixingf, scoref, head, group, q, k, v, args...)</code></pre><p>Generic version <a href="#NeuralAttentionlib.grouped_query_attention"><code>grouped_query_attention</code></a>. Need to specify mixing and score functon.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L44-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.generic_multihead_qkv_attention" href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>NeuralAttentionlib.generic_multihead_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generic_multihead_qkv_attention(mixingf, scoref, head, q, k, v, args...)</code></pre><p>Generic version of <a href="#NeuralAttentionlib.multihead_qkv_attention"><code>multihead_qkv_attention</code></a>. Need to specify mixing and score function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L30-L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.generic_qkv_attention" href="#NeuralAttentionlib.generic_qkv_attention"><code>NeuralAttentionlib.generic_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generic_qkv_attention(mixingf, scoref, q, k, v, args...)</code></pre><p>Generic version of <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>. Need to specify mixing and score function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.get_sincos_position_embeddings" href="#NeuralAttentionlib.get_sincos_position_embeddings"><code>NeuralAttentionlib.get_sincos_position_embeddings</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_sincos_position_embeddings(hidden_size::Integer, normalized::Bool, x)</code></pre><p>sincos position embeddings. <code>x</code> can be either a integer specifying the length or an array of position indices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L223-L227">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.grouped_query_attention" href="#NeuralAttentionlib.grouped_query_attention"><code>NeuralAttentionlib.grouped_query_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">grouped_query_attention(head, group, q, k, v, mask=nothing)</code></pre><p>Similar to <a href="#NeuralAttentionlib.multihead_qkv_attention"><code>multihead_qkv_attention</code></a>, but multiple queries are using the same group of keys/values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L51-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.layer_norm" href="#NeuralAttentionlib.layer_norm"><code>NeuralAttentionlib.layer_norm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">layer_norm([epsilon = 1e-5,] alpha, beta, x)</code></pre><p>Function which perform layer normalization on <code>x</code>. <code>alpha</code> and <code>beta</code> can a <code>Vector</code>, <code>Number</code> or <code>Nothing</code>.</p><p><span>$layer_norm(α, β, x) = α\frac{(x - μ)}{σ} + β$</span></p><p>If both <code>alpha</code> and <code>beta</code> is <code>Nothing</code>, this is just a standardize function applied on the first dimension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L308-L316">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.masked_score" href="#NeuralAttentionlib.masked_score"><code>NeuralAttentionlib.masked_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">masked_score(mask) = masked_score $ mask
masked_score(maskop, mask) = masked_score $ maskop $ mask
masked_score(maskop::AbstractMaskOp, mask::AbstractMask, score, args...)</code></pre><p>Masked attention score api. Applying the <code>mask</code> according to <code>maskop</code> on the attention score  compute from <code>score(args...)</code>.</p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>, <a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L126-L135">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.merge_head" href="#NeuralAttentionlib.merge_head"><code>NeuralAttentionlib.merge_head</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">merge_head(x)</code></pre><p>merge the <code>head</code> dimension split by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L301-L305">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.mixing" href="#NeuralAttentionlib.mixing"><code>NeuralAttentionlib.mixing</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mixing(f, v, g, args...) = f(attention_score(g, args...), v)</code></pre><p><code>Mixing</code> function api. Can be overload for doing custom implementation with <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>.  <code>f</code> is the mixing function and <code>g</code> is score function.</p><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a>, <a href="#NeuralAttentionlib.generic_multihead_qkv_attention"><code>generic_multihead_qkv_attention</code></a>, <a href="#NeuralAttentionlib.attention_score"><code>attention_score</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L88-L95">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_in" href="#NeuralAttentionlib.move_head_dim_in"><code>NeuralAttentionlib.move_head_dim_in</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_in(x::AbstractArray, nobatch=false)</code></pre><p>Equivanlent to <code>permutedims(x, move_head_dim_in_perm(x, nobatch)))</code></p><p>See also: <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_in_perm"><code>move_head_dim_in_perm</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L292-L298">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_in_perm" href="#NeuralAttentionlib.move_head_dim_in_perm"><code>NeuralAttentionlib.move_head_dim_in_perm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_in_perm(x::AbstractArray{T, N}, nobatch=false)
move_head_dim_in_perm(N::Int, nobatch=false)</code></pre><p>Dimension order for <code>permutedims</code> to move the <code>head</code> dimension (created by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>) from batch dimension  to feature dimension (for <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>). Return a tuple of integer of length <code>n</code>.  <code>nobatch</code> specify where <code>x</code> is a batch of data.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; Functional.move_head_dim_in_perm(5, false)
(1, 4, 2, 3, 5)

julia&gt; Functional.move_head_dim_in_perm(5, true)
(1, 5, 2, 3, 4)
</code></pre><p>See also: <a href="#NeuralAttentionlib.merge_head"><code>merge_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_in"><code>move_head_dim_in</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L269-L289">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_out" href="#NeuralAttentionlib.move_head_dim_out"><code>NeuralAttentionlib.move_head_dim_out</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_out(x::AbstractArray, nobatch=false)</code></pre><p>Equivanlent to <code>permutedims(x, move_head_dim_out_perm(x, nobatch)))</code></p><p>See also: <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_out_perm"><code>move_head_dim_out_perm</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L260-L266">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.move_head_dim_out_perm" href="#NeuralAttentionlib.move_head_dim_out_perm"><code>NeuralAttentionlib.move_head_dim_out_perm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">move_head_dim_out_perm(x::AbstractArray{T, N}, nobatch=false)
move_head_dim_out_perm(N::Int, nobatch=false)</code></pre><p>Dimension order for <code>permutedims</code> to move the <code>head</code> dimension (created by <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>) to batch dimension.  Return a tuple of integer of length <code>n</code>. <code>nobatch</code> specify where <code>x</code> is a batch of data.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; Functional.move_head_dim_out_perm(5, false)
(1, 3, 4, 2, 5)

julia&gt; Functional.move_head_dim_out_perm(5, true)
(1, 3, 4, 5, 2)
</code></pre><p>See also: <a href="#NeuralAttentionlib.split_head"><code>split_head</code></a>, <a href="#NeuralAttentionlib.move_head_dim_out"><code>move_head_dim_out</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L238-L257">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.multihead_qkv_attention" href="#NeuralAttentionlib.multihead_qkv_attention"><code>NeuralAttentionlib.multihead_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">multihead_qkv_attention(head, q, k, v, mask=nothing)</code></pre><p>Multihead version of <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a>. The core operation for implement a regular transformer layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L37-L41">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.naive_qkv_attention" href="#NeuralAttentionlib.naive_qkv_attention"><code>NeuralAttentionlib.naive_qkv_attention</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">naive_qkv_attention(q, k, v, mask=nothing)</code></pre><p>The scaled dot-product attention of a regular transformer layer.</p><p><span>$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</span></p><p>It&#39;s equivalent to <code>generic_qkv_attention(weighted_sum_mixing, normalized_score(NNlib.softmax) $ masked_score(GenericMaskOp(), mask) $ scaled_dot_product_score, q, k, v)</code>.</p><p>#Example</p><pre><code class="language-julia hljs">julia&gt; fdim, ldim, bdim = 32, 10, 4;

julia&gt; x = randn(fdim, ldim, bdim);

julia&gt; y = naive_qkv_attention(x, x, x); # simple self attention

# no mask here
julia&gt; z = generic_qkv_attention(weighted_sum_mixing, normalized_score(NNlib.softmax) $ scaled_dot_product_score, x, x, x);

julia&gt; y ≈ z
true
</code></pre><p>See also: <a href="#NeuralAttentionlib.generic_qkv_attention"><code>generic_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L58-L85">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.normalized_score" href="#NeuralAttentionlib.normalized_score"><code>NeuralAttentionlib.normalized_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">normalized_score(norm) = normalized_score $ norm
normalized_score(norm, score, args...)</code></pre><p>Normalized attenion score api. <code>norm</code> is the normalize function (like <code>softmax</code>) and <code>score</code> is the function  that compute attention score from <code>args...</code>.</p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L115-L123">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.rms_layer_norm" href="#NeuralAttentionlib.rms_layer_norm"><code>NeuralAttentionlib.rms_layer_norm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rms_layer_norm([epsilon = 1e-5,] alpha, x)</code></pre><p>Function which perform root-mean-square layer normalization on <code>x</code>. <code>alpha</code> and <code>beta</code> can a <code>Vector</code>, <code>Number</code>  or <code>Nothing</code>.</p><p><span>$rms_layer_norm(α, x) = α\frac{x}{\sqrt{\sum_{i=1}^{N} x^2 / N}}$</span></p><p>If both <code>alpha</code> is <code>Nothing</code>, this is just a normalization with root-mean-square function applied on the first  dimension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L319-L329">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.scalar_relative_position_embedding" href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>NeuralAttentionlib.scalar_relative_position_embedding</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">scalar_relative_position_embedding(relative_position_id_func, embedding_table, score, args...)</code></pre><p>A relative position embedding that produce a trainable scalar bias for each value in the attention score.  <code>relative_position_id_func</code> is a function that take the attention score and return a <code>relative_position_id</code>  matrix with the same size of the attention score with batches (normally <code>(key length, query length)</code>). This  <code>relative_position_id</code> would be used to index (or <code>gather</code>) the <code>embedding_table</code>. <code>embedding_table</code> is an  array with multiple dimensions, where the first dimension is the number of possible <code>&quot;id&quot;</code>s and the remaining  dimensions are for giving different value to each heads. By default we treat the last dimension of attention  score as the batch dimension and the dimension between last dimension and the &quot;length&quot; dimension as the head  dimensions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L171-L182">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.scaled_dot_product_score" href="#NeuralAttentionlib.scaled_dot_product_score"><code>NeuralAttentionlib.scaled_dot_product_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs"> scaled_dot_product_score(q, k, s = sqrt(inv(size(k, 1))))</code></pre><p>The scaled dot-product attention score function of a regular transformer layer.</p><p><span>$Score(Q, K) = \frac{QK^T}{\sqrt{d_k}}$</span></p><pre><code class="nohighlight hljs">scaled_dot_product_score(f, q, k)</code></pre><p>Apply a transform function <code>f</code> on <code>q</code>/<code>k</code> before dot-product.</p><p>See also: <a href="#NeuralAttentionlib.naive_qkv_attention"><code>naive_qkv_attention</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L138-L150">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.split_head" href="#NeuralAttentionlib.split_head"><code>NeuralAttentionlib.split_head</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">split_head(head::Int, x)</code></pre><p>Split the first dimension into <code>head</code> piece of small vector. Equivalent to  <code>reshape(x, :, head, tail(size(x))...)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L230-L235">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.t5_bucketed_position_id" href="#NeuralAttentionlib.t5_bucketed_position_id"><code>NeuralAttentionlib.t5_bucketed_position_id</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">t5_bucketed_position_id(n_buckets::Int, max_distance::Int)</code></pre><p>A <code>relative_position_id_func</code> used in the T5 Transformer model. The relative distances is assigned to a  logarithmical buecket and the distance beyond <code>max_distance</code> would be assigned to the same bucket.</p><p>See also: <a href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>scalar_relative_position_embedding</code></a>, <a href="#NeuralAttentionlib.t5_causal_bucketed_position_id"><code>t5_causal_bucketed_position_id</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L196-L203">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.t5_causal_bucketed_position_id" href="#NeuralAttentionlib.t5_causal_bucketed_position_id"><code>NeuralAttentionlib.t5_causal_bucketed_position_id</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">t5_causal_bucketed_position_id(n_buckets::Int, max_distance::Int)</code></pre><p>Same as <code>t5_bucketed_position_id</code> but only attent to past. Should be used with <a href="#NeuralAttentionlib.CausalMask"><code>CausalMask</code></a></p><p>See also: <a href="#NeuralAttentionlib.scalar_relative_position_embedding"><code>scalar_relative_position_embedding</code></a>, <a href="#NeuralAttentionlib.t5_bucketed_position_id"><code>t5_bucketed_position_id</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L206-L212">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.weighted_sum_mixing" href="#NeuralAttentionlib.weighted_sum_mixing"><code>NeuralAttentionlib.weighted_sum_mixing</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">weighted_sum_mixing(s, v)</code></pre><p>The mixing function of a regular transformer layer. <code>s</code> is the attention score and <code>v</code> is the value of QKV attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L98-L102">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.with_rotary_position_embedding" href="#NeuralAttentionlib.with_rotary_position_embedding"><code>NeuralAttentionlib.with_rotary_position_embedding</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">with_rotary_position_embedding([size,] x)</code></pre><p>Apply rotary position embedding to <code>x</code>. Can take an <code>size</code> argument and the rotary position embedding will only apply  to <code>x[1:size, :, ...]</code>. Should be used with <code>scaled_dot_product_score</code>/<code>dot_product_score</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/functional.jl#L215-L220">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.CausalGroupedQueryAttenOp" href="#NeuralAttentionlib.CausalGroupedQueryAttenOp"><code>NeuralAttentionlib.CausalGroupedQueryAttenOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct CausalGroupedQueryAttenOp{F} &lt;: AbstractAttenOp
    head::Int
    group::Int
    p::F
end</code></pre><p>Structure for holding parameter of <a href="#NeuralAttentionlib.grouped_query_attention"><code>grouped_query_attention</code></a>.</p><pre><code class="nohighlight hljs">(op::CausalGroupedQueryAttenOp)(q, k, v, mask = nothing)</code></pre><p>Perform grouped query attention where <code>mask</code> would be combined with a <a href="#NeuralAttentionlib.CausalMask"><code>CausalMask</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L163-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.CausalGroupedQueryAttenOpWithScore" href="#NeuralAttentionlib.CausalGroupedQueryAttenOpWithScore"><code>NeuralAttentionlib.CausalGroupedQueryAttenOpWithScore</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Same as <a href="#NeuralAttentionlib.CausalGroupedQueryAttenOp"><code>CausalGroupedQueryAttenOp</code></a> but also return the attention score</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L187">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.CausalMultiheadQKVAttenOp" href="#NeuralAttentionlib.CausalMultiheadQKVAttenOp"><code>NeuralAttentionlib.CausalMultiheadQKVAttenOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct CausalMultiheadQKVAttenOp{F} &lt;: AbstractAttenOp
    head::Int  # number of head
    p::F       # dropout probability
end</code></pre><p>Structure for holding parameter of <a href="#NeuralAttentionlib.multihead_qkv_attention"><code>multihead_qkv_attention</code></a>.</p><pre><code class="nohighlight hljs">(op::CausalMultiheadQKVAttenOp)(q, k, v, mask = nothing)</code></pre><p>Perform multihead attention where <code>mask</code> would be combined with a <a href="#NeuralAttentionlib.CausalMask"><code>CausalMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L112-L123">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.CausalMultiheadQKVAttenOpWithScore" href="#NeuralAttentionlib.CausalMultiheadQKVAttenOpWithScore"><code>NeuralAttentionlib.CausalMultiheadQKVAttenOpWithScore</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Same as <a href="#NeuralAttentionlib.CausalMultiheadQKVAttenOp"><code>CausalMultiheadQKVAttenOp</code></a> but also return the attention score</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L133">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.GroupedQueryAttenOp" href="#NeuralAttentionlib.GroupedQueryAttenOp"><code>NeuralAttentionlib.GroupedQueryAttenOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct GroupedQueryAttenOp{F} &lt;: AbstractAttenOp
    head::Int
    group::Int
    p::F
end</code></pre><p>Structure for holding parameter of <a href="#NeuralAttentionlib.grouped_query_attention"><code>grouped_query_attention</code></a>.</p><pre><code class="nohighlight hljs">(op::GroupedQueryAttenOp)(q, k, v, mask = nothing)</code></pre><p>Perform grouped query attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L136-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.GroupedQueryAttenOpWithScore" href="#NeuralAttentionlib.GroupedQueryAttenOpWithScore"><code>NeuralAttentionlib.GroupedQueryAttenOpWithScore</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Same as <a href="#NeuralAttentionlib.GroupedQueryAttenOp"><code>GroupedQueryAttenOp</code></a> but also return the attention score</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L160">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.MultiheadQKVAttenOp" href="#NeuralAttentionlib.MultiheadQKVAttenOp"><code>NeuralAttentionlib.MultiheadQKVAttenOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct MultiheadQKVAttenOp{F} &lt;: AbstractAttenOp
    head::Int  # number of head
    p::F       # dropout probability
end</code></pre><p>Structure for holding parameter of <a href="#NeuralAttentionlib.multihead_qkv_attention"><code>multihead_qkv_attention</code></a>.</p><pre><code class="nohighlight hljs">(op::MultiheadQKVAttenOp)(q, k, v, mask = nothing)</code></pre><p>Perform multihead attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L87-L99">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.MultiheadQKVAttenOpWithScore" href="#NeuralAttentionlib.MultiheadQKVAttenOpWithScore"><code>NeuralAttentionlib.MultiheadQKVAttenOpWithScore</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Same as <a href="#NeuralAttentionlib.MultiheadQKVAttenOp"><code>MultiheadQKVAttenOp</code></a> but also return the attention score</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/types.jl#L109">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.PrefixedFunction" href="#NeuralAttentionlib.PrefixedFunction"><code>NeuralAttentionlib.PrefixedFunction</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PrefixedFunction(f, args::NTuple{N}) &lt;: Function</code></pre><p>A type representating a partially-applied version of the function <code>f</code>, with the first <code>N</code> arguments fixed to the  values <code>args</code>. In other words, <code>PrefixedFunction(f, args)</code> behaves similarly to <code>(xs...)-&gt;f(args..., xs...)</code>.</p><p>See also <a href="#NeuralAttentionlib.:\$-Tuple{Function, Any}"><code>NeuralAttentionlib.:$</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/utils.jl#L7-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.:\$-Tuple{Function, Any}" href="#NeuralAttentionlib.:\$-Tuple{Function, Any}"><code>NeuralAttentionlib.:$</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">f $ x
f $ x $ y $ ...</code></pre><p>Partially-applied function. Return a <a href="#NeuralAttentionlib.PrefixedFunction"><code>PrefixedFunction</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/utils.jl#L28-L33">source</a></section></article><h2 id="Mask"><a class="docs-heading-anchor" href="#Mask">Mask</a><a id="Mask-1"></a><a class="docs-heading-anchor-permalink" href="#Mask" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.AbstractMaskOp" href="#NeuralAttentionlib.AbstractMaskOp"><code>NeuralAttentionlib.AbstractMaskOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractMaskOp</code></pre><p>Trait-like abstract type for holding operation related argument, defined how the mask should be apply to input array</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/mask/mask.jl#L34-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::GenericMaskOp, mask::AbstractMask, score)</code></pre><p>Equivalent to <code>op.apply(score, op.scale .* (op.flip ? .! mask : mask))</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(10, 10);

julia&gt; m = CausalMask()
CausalMask()

julia&gt; apply_mask(GenericMaskOp(.+, true, -1e9), m, x) ==  @. x + (!m * -1e9)
true
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/mask/mask.jl#L75-L92">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveMaskOp, NeuralAttentionlib.AbstractMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::NaiveMaskOp, mask::AbstractMask, score)</code></pre><p>Directly broadcast multiply mask to attention score, i.e. <code>score .* mask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/mask/mask.jl#L47-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.AbstractArrayMask" href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractArrayMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask with array data</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L63-L67">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.AbstractAttenMask" href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractAttenMask &lt;: AbstractMask</code></pre><p>Abstract type for mask data specifically for attention.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L49-L53">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.AbstractDatalessMask" href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractDatalessMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask without array data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L56-L60">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.AbstractMask" href="#NeuralAttentionlib.AbstractMask"><code>NeuralAttentionlib.AbstractMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractMask</code></pre><p>Abstract type for mask data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L35-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.AbstractSeqMask" href="#NeuralAttentionlib.AbstractSeqMask"><code>NeuralAttentionlib.AbstractSeqMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractSeqMask &lt;: AbstractMask</code></pre><p>Abstract type for mask data specifically for sequence.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L42-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.BandPartMask" href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BandPartMask(l::Int, u::Int) &lt;: AbstractAttenMask{DATALESS}</code></pre><p>Attention mask that only allow <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/band_part">band_part</a>  values to pass.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; trues(10, 10) .* BandPartMask(3, 5)
10×10 BitMatrix:
 1  1  1  1  1  1  0  0  0  0
 1  1  1  1  1  1  1  0  0  0
 1  1  1  1  1  1  1  1  0  0
 1  1  1  1  1  1  1  1  1  0
 0  1  1  1  1  1  1  1  1  1
 0  0  1  1  1  1  1  1  1  1
 0  0  0  1  1  1  1  1  1  1
 0  0  0  0  1  1  1  1  1  1
 0  0  0  0  0  1  1  1  1  1
 0  0  0  0  0  0  1  1  1  1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L181-L203">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.BatchedMask" href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BatchedMask(mask::AbstractMask) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for applying the same mask within the same batch.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 2, 2) .* m
ERROR: DimensionMismatch(&quot;arrays could not be broadcast to a common size; mask require ndims(A) == 3&quot;)
Stacktrace:
[...]

julia&gt; trues(3,3, 2, 2) .* BatchedMask(m) # 4-th dim become batch dim
3×3×2×2 BitArray{4}:
[:, :, 1, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 1, 2] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 2, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L532-L583">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.BiLengthMask" href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BiLengthMask(q_len::A, k_len::A) where {A &lt;: AbstractArray{Int, N}} &lt;: AbstractAttenMask{ARRAYDATA}</code></pre><p>Attention mask specified by two arrays of integer that indicate the length dimension size.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; bm = BiLengthMask([2,3], [3, 5])
BiLengthMask{1, Vector{Int32}}(Int32[2, 3], Int32[3, 5])

julia&gt; trues(5,5, 2) .* bm
5×5×2 BitArray{3}:
[:, :, 1] =
 1  1  0  0  0
 1  1  0  0  0
 1  1  0  0  0
 0  0  0  0  0
 0  0  0  0  0

[:, :, 2] =
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
</code></pre><p>See also: <a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BiSeqMask"><code>BiSeqMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L273-L303">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.BiSeqMask" href="#NeuralAttentionlib.BiSeqMask"><code>NeuralAttentionlib.BiSeqMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BiSeqMask(qmask::A1, kmask::A2) where {A1 &lt;: AbstractSeqMask, A2 &lt;: AbstractSeqMask} &lt;: AbstractAttenMask</code></pre><p>Take two sequence mask and construct an attention mask.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; trues(7, 7, 2) .* Masks.BiSeqMask(Masks.LengthMask([3, 5]), Masks.RevLengthMask([3, 5]))
7×7×2 BitArray{3}:
[:, :, 1] =
 0  0  0  0  0  0  0
 0  0  0  0  0  0  0
 0  0  0  0  0  0  0
 0  0  0  0  0  0  0
 1  1  1  0  0  0  0
 1  1  1  0  0  0  0
 1  1  1  0  0  0  0

[:, :, 2] =
 0  0  0  0  0  0  0
 0  0  0  0  0  0  0
 1  1  1  1  1  0  0
 1  1  1  1  1  0  0
 1  1  1  1  1  0  0
 1  1  1  1  1  0  0
 1  1  1  1  1  0  0</code></pre><p>See also: <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>, <a href="#NeuralAttentionlib.RevBiLengthMask"><code>RevBiLengthMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L477-L507">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.CausalMask" href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CausalMask() &lt;: AbstractAttenMask{DATALESS}</code></pre><p>Attention mask that block the future values.</p><p>Similar to applying <code>LinearAlgebra.triu!</code> on the score matrix</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; trues(10, 10) .* CausalMask()
10×10 BitMatrix:
 1  1  1  1  1  1  1  1  1  1
 0  1  1  1  1  1  1  1  1  1
 0  0  1  1  1  1  1  1  1  1
 0  0  0  1  1  1  1  1  1  1
 0  0  0  0  1  1  1  1  1  1
 0  0  0  0  0  1  1  1  1  1
 0  0  0  0  0  0  1  1  1  1
 0  0  0  0  0  0  0  1  1  1
 0  0  0  0  0  0  0  0  1  1
 0  0  0  0  0  0  0  0  0  1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L89-L112">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.GenericAttenMask" href="#NeuralAttentionlib.GenericAttenMask"><code>NeuralAttentionlib.GenericAttenMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GenericAttenMask &lt;: AbstractAttenMask{ARRAYDATA}</code></pre><p>Generic attention mask. Just a wrapper over <code>AbstractArray{Bool}</code> for dispatch.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; bitmask = rand(Bool, 10, 10)
10×10 Matrix{Bool}:
 1  0  1  1  0  0  1  0  1  1
 0  0  1  1  0  0  0  1  1  1
 0  1  0  1  0  1  0  0  1  0
 0  1  1  0  1  1  0  0  0  1
 1  0  1  1  1  0  0  0  0  1
 1  0  1  0  1  1  1  1  0  1
 0  0  0  1  1  1  0  1  1  1
 1  0  1  0  1  1  1  0  0  1
 0  1  0  1  0  0  1  1  0  1
 0  0  0  1  0  1  0  0  0  1

julia&gt; trues(10, 10) .* GenericAttenMask(bitmask)
10×10 BitMatrix:
 1  0  1  1  0  0  1  0  1  1
 0  0  1  1  0  0  0  1  1  1
 0  1  0  1  0  1  0  0  1  0
 0  1  1  0  1  1  0  0  0  1
 1  0  1  1  1  0  0  0  0  1
 1  0  1  0  1  1  1  1  0  1
 0  0  0  1  1  1  0  1  1  1
 1  0  1  0  1  1  1  0  0  1
 0  1  0  1  0  0  1  1  0  1
 0  0  0  1  0  1  0  0  0  1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L206-L240">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.GenericSeqMask" href="#NeuralAttentionlib.GenericSeqMask"><code>NeuralAttentionlib.GenericSeqMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GenericSeqMask(mask::AbstractArray{Bool}) &lt;: AbstractSeqMask{ARRAYDATA}</code></pre><p>Create a sequence mask from an array of <code>Bool</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = GenericSeqMask(rand(Bool, 10, 2))
GenericSeqMask{3, Array{Bool, 3}}([0 1 … 0 0;;; 1 0 … 1 0])

julia&gt; trues(7, 10, 2) .* m
7×10×2 BitArray{3}:
[:, :, 1] =
 0  1  0  0  1  0  0  0  0  0
 0  1  0  0  1  0  0  0  0  0
 0  1  0  0  1  0  0  0  0  0
 0  1  0  0  1  0  0  0  0  0
 0  1  0  0  1  0  0  0  0  0
 0  1  0  0  1  0  0  0  0  0
 0  1  0  0  1  0  0  0  0  0

[:, :, 2] =
 1  0  1  1  0  1  1  1  1  0
 1  0  1  1  0  1  1  1  1  0
 1  0  1  1  0  1  1  1  1  0
 1  0  1  1  0  1  1  1  1  0
 1  0  1  1  0  1  1  1  1  0
 1  0  1  1  0  1  1  1  1  0
 1  0  1  1  0  1  1  1  1  0

julia&gt; m.mask
1×10×2 Array{Bool, 3}:
[:, :, 1] =
 0  1  0  0  1  0  0  0  0  0

[:, :, 2] =
 1  0  1  1  0  1  1  1  1  0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L434-L474">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.Indexer" href="#NeuralAttentionlib.Indexer"><code>NeuralAttentionlib.Indexer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Indexer(m::AbstractMask, size::Dims{N}) &lt;: AbstractArray{Bool, N}
Indexer(m::AbstractMask, size::Dims{N}, scale::T) &lt;: AbstractArray{T, N}</code></pre><p>A lazy array-like object that &quot;materialize&quot; the mask <code>m</code> with <code>size</code> and a optional <code>scale</code> without size check.</p><p>See also: <a href="#NeuralAttentionlib.GetIndexer"><code>GetIndexer</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L18-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.LengthMask" href="#NeuralAttentionlib.LengthMask"><code>NeuralAttentionlib.LengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LengthMask(len::AbstractArray{Int, N}) &lt;: AbstractSeqMask{ARRAYDATA}</code></pre><p>A Sequence Mask specified by an array of integer that indicate the length dimension size.  Can be convert to attention mask (<a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>) with <a href="#NeuralAttentionlib.AttenMask"><code>AttenMask</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; ones(7, 7, 2) .* LengthMask([3, 5])
7×7×2 Array{Float64, 3}:
[:, :, 1] =
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0  0.0  0.0

[:, :, 2] =
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  1.0  0.0  0.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L306-L336">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.LocalMask" href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LocalMask(width::Int) &lt;: AbstractAttenMask{DATALESS}</code></pre><p>Attention mask that only allow local (diagonal like) values to pass.</p><p><code>width</code> should be ≥ 0 and <code>A .* LocalMask(1)</code> is similar to <code>Diagonal(A)</code></p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; trues(10, 10) .* LocalMask(3)
10×10 BitMatrix:
 1  1  1  0  0  0  0  0  0  0
 1  1  1  1  0  0  0  0  0  0
 1  1  1  1  1  0  0  0  0  0
 0  1  1  1  1  1  0  0  0  0
 0  0  1  1  1  1  1  0  0  0
 0  0  0  1  1  1  1  1  0  0
 0  0  0  0  1  1  1  1  1  0
 0  0  0  0  0  1  1  1  1  1
 0  0  0  0  0  0  1  1  1  1
 0  0  0  0  0  0  0  1  1  1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L115-L138">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.NoMask" href="#NeuralAttentionlib.NoMask"><code>NeuralAttentionlib.NoMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NoMask{T}() &lt;: AbstractDatalessMask{T}</code></pre><p>A mask for no mask only for work with wrapper masks type constraints. Generally use <code>nothing</code> instead of <code>NoMask</code>  with <code>apply_mask</code>/<code>mask_score</code> for the fast path.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L81-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.RandomMask" href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomMask(p::Float32) &lt;: AbstractAttenMask{DATALESS}</code></pre><p>Attention mask that block value randomly.</p><p><code>p</code> specify the percentage of value to block. e.g. <code>A .* RandomMask(0)</code> is equivalent to <code>identity(A)</code> and  <code>A .* RandomMask(1)</code> is equivalent to <code>zero(A)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; trues(10, 10) .* RandomMask(0.5)
10×10 BitMatrix:
 1  1  1  1  1  1  0  1  1  1
 0  0  1  0  1  0  0  0  1  0
 0  0  1  1  0  0  0  0  1  1
 1  0  1  1  1  0  0  1  0  1
 1  1  0  1  0  0  1  0  1  1
 0  1  1  1  1  0  1  0  1  1
 1  1  0  0  0  0  1  0  0  0
 0  0  1  0  1  1  0  1  1  0
 1  1  1  1  1  1  0  0  1  1
 0  0  1  0  1  1  0  0  1  0

julia&gt; trues(10, 10) .* RandomMask(0.5)
10×10 BitMatrix:
 1  0  1  1  0  0  1  1  0  1
 0  1  0  1  1  1  0  0  1  1
 0  0  1  0  0  0  1  1  0  0
 0  0  0  0  1  0  0  1  1  1
 0  1  1  1  1  0  1  0  0  1
 1  0  0  1  1  0  0  0  1  1
 1  1  1  0  1  1  1  0  0  0
 0  0  1  1  0  0  1  1  1  0
 0  1  1  1  1  0  1  0  1  0
 0  0  1  0  0  0  0  1  1  1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L141-L178">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.RepeatMask" href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RepeatMask(mask::AbstractMask, num::Int) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for doing inner repeat on the last dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 4) .* m
ERROR: DimensionMismatch(&quot;arrays could not be broadcast to a common size; mask require 3-th dimension to be 2, but get 4&quot;)
Stacktrace:
[...]

julia&gt; trues(3,3, 4) .* RepeatMask(m, 2)
3×3×4 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 3] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 4] =
 1  1  1
 1  1  1
 1  1  1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L586-L637">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.RevBiLengthMask" href="#NeuralAttentionlib.RevBiLengthMask"><code>NeuralAttentionlib.RevBiLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RevBiLengthMask(q_len::A, k_len::A) where {A &lt;: AbstractArray{Int, N}} &lt;: AbstractAttenMask{ARRAYDATA}</code></pre><p><a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a> but counts from the end of array, used for left padding.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; bm = RevBiLengthMask([2,3], [3, 5])
RevBiLengthMask{1, Vector{Int32}}(Int32[2, 3], Int32[3, 5])

julia&gt; trues(5,5, 2) .* bm
5×5×2 BitArray{3}:
[:, :, 1] =
 0  0  0  0  0
 0  0  0  0  0
 0  0  0  1  1
 0  0  0  1  1
 0  0  0  1  1

[:, :, 2] =
 0  0  1  1  1
 0  0  1  1  1
 0  0  1  1  1
 0  0  1  1  1
 0  0  1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.RevLengthMask"><code>RevLengthMask</code></a>, <a href="#NeuralAttentionlib.RevSymLengthMask"><code>RevSymLengthMask</code></a>, <a href="#NeuralAttentionlib.BiSeqMask"><code>BiSeqMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L368-L398">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.RevLengthMask" href="#NeuralAttentionlib.RevLengthMask"><code>NeuralAttentionlib.RevLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RevLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractSeqMask{ARRAYDATA}</code></pre><p><a href="#NeuralAttentionlib.LengthMask"><code>LengthMask</code></a> but counts from the end of array, used for left padding.  Can be convert to attention mask (<a href="#NeuralAttentionlib.RevSymLengthMask"><code>RevSymLengthMask</code></a>, <a href="#NeuralAttentionlib.RevBiLengthMask"><code>RevBiLengthMask</code></a>) with <a href="#NeuralAttentionlib.AttenMask"><code>AttenMask</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; ones(7, 7, 2) .* RevLengthMask([3, 5])
7×7×2 Array{Float64, 3}:
[:, :, 1] =
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0
 0.0  0.0  0.0  0.0  1.0  1.0  1.0

[:, :, 2] =
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
 0.0  0.0  1.0  1.0  1.0  1.0  1.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L401-L431">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.RevSymLengthMask" href="#NeuralAttentionlib.RevSymLengthMask"><code>NeuralAttentionlib.RevSymLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RevSymLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractAttenMask{ARRAYDATA}</code></pre><p><a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a> but counts from the end of array, used for left padding.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = RevSymLengthMask([2,3])
RevSymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 0  0  0
 0  1  1
 0  1  1

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L339-L365">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.SymLengthMask" href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractAttenMask{ARRAYDATA}</code></pre><p>Attention mask specified by an array of integer that indicate the length dimension size.  assuming <em>Query</em> length and <em>Key</em> length are the same.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.LengthMask"><code>LengthMask</code></a>, <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L243-L270">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.:!-Tuple{NeuralAttentionlib.AbstractMask}" href="#Base.:!-Tuple{NeuralAttentionlib.AbstractMask}"><code>Base.:!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">!m::AbstractMask</code></pre><p>Boolean not of an attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L510-L514">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}" href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:&amp;</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractMask &amp; m2::AbstractMask</code></pre><p>logical and of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L525-L529">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}" href="#Base.:|-Tuple{NeuralAttentionlib.AbstractMask, NeuralAttentionlib.AbstractMask}"><code>Base.:|</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractMask | m2::AbstractMask</code></pre><p>logical or of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L517-L521">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.AttenMask" href="#NeuralAttentionlib.AttenMask"><code>NeuralAttentionlib.AttenMask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AttenMask(m::AbstractMask)</code></pre><p>Convert mask into corresponding attention mask.</p><pre><code class="nohighlight hljs">AttenMask(q_mask::AbstractSeqMask, k_mask::AbstractSeqMask)</code></pre><p>Create a attention mask from 2 sequence masks specific the sequence mask for &quot;query&quot; and &quot;key&quot;.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L70-L78">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.GetIndexer" href="#NeuralAttentionlib.GetIndexer"><code>NeuralAttentionlib.GetIndexer</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">GetIndexer(m::AbstractMask, destsize::Dims{N})</code></pre><p>Return the <a href="#NeuralAttentionlib.Indexer"><code>Indexer</code></a> of <code>m</code> and check if the mask <code>m</code> can be applied to an array with size <code>destsize</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L28-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.getmask" href="#NeuralAttentionlib.getmask"><code>NeuralAttentionlib.getmask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">getmask(m::AbstractMask, score, scale = 1)</code></pre><p>Convert <code>m</code> into mask array of <code>AbstractArray</code> for <code>score</code> with <code>scale</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; getmask(CausalMask(), randn(7,7), 2)
7×7 Matrix{Float64}:
 2.0  2.0  2.0  2.0  2.0  2.0  2.0
 0.0  2.0  2.0  2.0  2.0  2.0  2.0
 0.0  0.0  2.0  2.0  2.0  2.0  2.0
 0.0  0.0  0.0  2.0  2.0  2.0  2.0
 0.0  0.0  0.0  0.0  2.0  2.0  2.0
 0.0  0.0  0.0  0.0  0.0  2.0  2.0
 0.0  0.0  0.0  0.0  0.0  0.0  2.0
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L640-L659">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.lengths" href="#NeuralAttentionlib.lengths"><code>NeuralAttentionlib.lengths</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">lengths(::AbstractSeqMask)</code></pre><p>Get the number of <code>true</code>s of each batch in the sequence mask.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/mask.jl#L662-L666">source</a></section></article><h2 id="Matmul"><a class="docs-heading-anchor" href="#Matmul">Matmul</a><a id="Matmul-1"></a><a class="docs-heading-anchor-permalink" href="#Matmul" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.CollapsedDimsArray" href="#NeuralAttentionlib.CollapsedDimsArray"><code>NeuralAttentionlib.CollapsedDimsArray</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CollapsedDimsArray{T}(array, ni::Integer, nj::Integer) &lt;: AbstractArray{T, 3}</code></pre><p>Similar to lazy reshape array with <a href="#NeuralAttentionlib.collapsed_size"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L9-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.collapsed_size" href="#NeuralAttentionlib.collapsed_size"><code>NeuralAttentionlib.collapsed_size</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">collapsed_size(x, ni, nj [, n])::Dim{3}</code></pre><p>Collapse the dimensionality of <code>x</code> into 3 according to <code>ni</code> and <code>nj</code> where <code>ni</code>, <code>nj</code> specify the number of  second and third dimensions it take.</p><pre><code class="nohighlight hljs">(X1, X2, ..., Xk, Xk+1, Xk+2, ..., Xk+ni, Xk+ni+1, ..., Xn)
 |______dim1___|  |_________ni_________|  |______nj______|</code></pre><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(7,6,5,4,3,2);

julia&gt; collapsed_size(x, 2, 2, 1)
42

julia&gt; collapsed_size(x, 2, 2, 2)
20

julia&gt; collapsed_size(x, 2, 2, 3)
6

julia&gt; collapsed_size(x, 2, 2)
(42, 20, 6)
</code></pre><p>See also: <a href="#NeuralAttentionlib.noncollapsed_size"><code>noncollapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L49-L78">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}" href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>NeuralAttentionlib.collapseddims</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">collapseddims(x::AbstractArray, xi, xj)</code></pre><p>Reshape <code>x</code> into 3 dim array, equivalent to <code>reshape(x, collapsed_size(x, xi, xj))</code></p><p>See also: <a href="#NeuralAttentionlib.collapsed_size"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L81-L87">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}" href="#NeuralAttentionlib.collapseddims-Tuple{NeuralAttentionlib.CollapsedDimsArray}"><code>NeuralAttentionlib.collapseddims</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">collapseddims(ca::CollapsedDimsArray)</code></pre><p>remove the wrapper and really reshape it.</p><p>See also: <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a>, <a href="#NeuralAttentionlib.unwrap_collapse"><code>unwrap_collapse</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L90-L96">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.matmul" href="#NeuralAttentionlib.matmul"><code>NeuralAttentionlib.matmul</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">matmul(a::AbstractArray, b::AbstractArray, s::Number = 1)</code></pre><p>Equivalent to <code>s .* (a * b)</code> if <code>a</code> and <code>b</code> are <code>Vector</code> or <code>Matrix</code>. For array with higher dimension,  it will convert <code>a</code> and <code>b</code> to <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a> and perform batched matrix multiplication, and then  return the result as <code>CollapsedDimsArray</code>. This is useful for preserving the dimensionality. If the batch dimension  of <code>a</code> and <code>b</code> have different shape, it pick the shape of <code>b</code> for batch dimension. Work with <code>NNlib.batch_transpose</code>  and <code>NNlib.batch_adjoint</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># b-dim shape: (6,)
julia&gt; a = CollapsedDimsArray(randn(3,4,2,3,6), 2, 1); size(a)
(12, 6, 6)

# b-dim shape: (3,1,2)
julia&gt; b = CollapsedDimsArray(randn(6,2,3,1,2), 1, 3); size(b)
(6, 2, 6)

julia&gt; c = matmul(a, b); size(c), typeof(c)
((12, 2, 6), CollapsedDimsArray{Float64, Array{Float64, 6}, Static.StaticInt{1}, Static.StaticInt{3}})

# b-dim shape: (3,1,2)
julia&gt; d = unwrap_collapse(c); size(d), typeof(d)
((3, 4, 2, 3, 1, 2), Array{Float64, 6})

# equivanlent to `batched_mul` but preserve shape
julia&gt; NNlib.batched_mul(collapseddims(a), collapseddims(b)) == collapseddims(matmul(a, b))
true
</code></pre><p>See also: <a href="#NeuralAttentionlib.CollapsedDimsArray"><code>CollapsedDimsArray</code></a>, <a href="#NeuralAttentionlib.unwrap_collapse"><code>unwrap_collapse</code></a>, <a href="#NeuralAttentionlib.collapseddims-Tuple{AbstractArray, Any, Any}"><code>collapseddims</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L106-L140">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.noncollapsed_size" href="#NeuralAttentionlib.noncollapsed_size"><code>NeuralAttentionlib.noncollapsed_size</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">noncollapsed_size(x, ni, nj [, n])</code></pre><p>Collapse the dimensionality of <code>x</code> into 3 according to <code>ni</code> and <code>nj</code>.</p><pre><code class="nohighlight hljs">(X1, X2, ..., Xk, Xk+1, Xk+2, ..., Xk+ni, Xk+ni+1, ..., Xn)
 |______dim1___|  |_________ni_________|  |______nj______|</code></pre><p>But take the size before collapse. e.g. <code>noncollapsed_size(x, ni, nj, 2)</code> will be <code>(Xi, Xi+1, ..., Xj-1)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(7,6,5,4,3,2);

julia&gt; noncollapsed_size(x, 2, 2, 1)
(7, 6)

julia&gt; noncollapsed_size(x, 2, 2, 2)
(5, 4)

julia&gt; noncollapsed_size(x, 2, 2, 3)
(3, 2)

julia&gt; noncollapsed_size(x, 2, 2)
((7, 6), (5, 4), (3, 2))
</code></pre><p>See also: <a href="#NeuralAttentionlib.collapsed_size"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L16-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.scaled_matmul" href="#NeuralAttentionlib.scaled_matmul"><code>NeuralAttentionlib.scaled_matmul</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">scaled_matmul(a::AbstractArray, b::AbstractArray, s::Number = 1)</code></pre><p>Basically equivalent to <code>unwrap_collapse(matmul(a, b, s))</code>, but not differentiable w.r.t. to <code>s</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L143-L147">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralAttentionlib.unwrap_collapse" href="#NeuralAttentionlib.unwrap_collapse"><code>NeuralAttentionlib.unwrap_collapse</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">unwrap_collapse(ca::CollapsedDimsArray)</code></pre><p>Return the underlying array of <code>CollapsedDimsArray</code>, otherwise just return the input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/498472ce81d51299027bc56f5bc79fd16a7081f5/src/module/matmul.jl#L99-L103">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example/">« Example</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Tuesday 4 June 2024 14:14">Tuesday 4 June 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
