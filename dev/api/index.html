<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · NeuralAttentionlib.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://chengchingwen.github.io/NeuralAttentionlib.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">NeuralAttentionlib.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../term/">Terminology</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Mask"><span>Mask</span></a></li><li><a class="tocitem" href="#Matmul"><span>Matmul</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/master/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><ul><li><a href="#Base.:!-Tuple{NeuralAttentionlib.AbstractAttenMask}"><code>Base.:!</code></a></li><li><a href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}"><code>Base.:&amp;</code></a></li><li><a href="#Base.:|-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}"><code>Base.:|</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a></li><li><a href="#NeuralAttentionlib.collapsed_size-Tuple{Any, Any, Any}"><code>NeuralAttentionlib.collapsed_size</code></a></li><li><a href="#NeuralAttentionlib.noncollapsed_size-NTuple{4, Any}"><code>NeuralAttentionlib.noncollapsed_size</code></a></li><li><a href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a></li><li><a href="#NeuralAttentionlib.AbstractAttenMaskOp"><code>NeuralAttentionlib.AbstractAttenMaskOp</code></a></li><li><a href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a></li><li><a href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a></li><li><a href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a></li><li><a href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a></li><li><a href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a></li><li><a href="#NeuralAttentionlib.CollapsedDimArray"><code>NeuralAttentionlib.CollapsedDimArray</code></a></li><li><a href="#NeuralAttentionlib.GenericMask"><code>NeuralAttentionlib.GenericMask</code></a></li><li><a href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a></li><li><a href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a></li><li><a href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a></li><li><a href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a></li></ul><h2 id="Mask"><a class="docs-heading-anchor" href="#Mask">Mask</a><a id="Mask-1"></a><a class="docs-heading-anchor-permalink" href="#Mask" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractAttenMask" href="#NeuralAttentionlib.AbstractAttenMask"><code>NeuralAttentionlib.AbstractAttenMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractAttenMask</code></pre><p>Abstract type for mask data, can be viewed as <code>AbstractArray{Bool}</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/mask/mask.jl#L8-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractAttenMaskOp" href="#NeuralAttentionlib.AbstractAttenMaskOp"><code>NeuralAttentionlib.AbstractAttenMaskOp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractAttenMaskOp</code></pre><p>Trait-like abstract type for holding operation related argument, defined how the mask should be apply to input array</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/mask/mask.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::GenericAttenMaskOp, mask::AbstractAttenMask, score)</code></pre><p>Equivalent to <code>op.apply(score, op.scale .* (op.flip ? .! mask : mask))</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; x = randn(10, 10);

julia&gt; m = CausalMask()
CausalMask()

julia&gt; apply_mask(GenericAttenMaskOp(.+, true, -1e9), m, x) ==  @. x + (!m * -1e9)
true
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/mask/mask.jl#L58-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}" href="#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}"><code>NeuralAttentionlib.apply_mask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_mask(op::NaiveAttenMaskOp, mask::AbstractAttenMask, score)</code></pre><p>Directly broadcast multiply mask to attention score, i.e. <code>score .* mask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/mask/mask.jl#L24-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractArrayMask" href="#NeuralAttentionlib.AbstractArrayMask"><code>NeuralAttentionlib.AbstractArrayMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractArrayMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask with array data</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L21-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.AbstractDatalessMask" href="#NeuralAttentionlib.AbstractDatalessMask"><code>NeuralAttentionlib.AbstractDatalessMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractDatalessMask &lt;: AbstractAttenMask</code></pre><p>Abstract type for mask without array data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L14-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BandPartMask" href="#NeuralAttentionlib.BandPartMask"><code>NeuralAttentionlib.BandPartMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BandPartMask(l::Int, u::Int) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that only allow <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/band_part">band_part</a>  values to pass.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L56-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BatchedMask" href="#NeuralAttentionlib.BatchedMask"><code>NeuralAttentionlib.BatchedMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BatchedMask(mask::AbstractArrayMask, batch_dim::Int) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for applying the same mask within the same batch.  Use <code>batch_dim</code> to specify from which dim should be treated as batch dims.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 2, 2) .* m
ERROR: [...]

julia&gt; trues(3,3, 2, 2) .* BatchedMask(m, 4) # 4-th dim is batch dim
3×3×2×2 BitArray{4}:
[:, :, 1, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 1, 2] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 2, 2] =
 1  1  1
 1  1  1
 1  1  1

# can also use negative value to count from the last
julia&gt; trues(3,3, 2, 2) .* BatchedMask(m, 4) == trues(3,3, 2, 2) .* BatchedMask(m, -1)
true
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L156-L210">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.BiLengthMask" href="#NeuralAttentionlib.BiLengthMask"><code>NeuralAttentionlib.BiLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BiLengthMask(q_len::A, k_len::A) where {A &lt;: AbstractArray{Int, N}} &lt;: AbstractArrayMask</code></pre><p>Attention mask specified by two arrays of integer that indicate the length dimension size.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; bm = BiLengthMask([2,3], [3, 5])
BiLengthMask{1, Vector{Int32}}(Int32[2, 3], Int32[3, 5])

julia&gt; trues(5,5, 2) .* bm
5×5×2 BitArray{3}:
[:, :, 1] =
 1  1  0  0  0
 1  1  0  0  0
 1  1  0  0  0
 0  0  0  0  0
 0  0  0  0  0

[:, :, 2] =
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
 1  1  1  0  0
</code></pre><p>See also: <a href="#NeuralAttentionlib.SymLengthMask"><code>SymLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L101-L131">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.CausalMask" href="#NeuralAttentionlib.CausalMask"><code>NeuralAttentionlib.CausalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CausalMask() &lt;: AbstractDatalessMask</code></pre><p>Attention mask that block the future values.</p><p>Similar to applying <code>LinearAlgebra.triu!</code> on the score matrix</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L28-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.GenericMask" href="#NeuralAttentionlib.GenericMask"><code>NeuralAttentionlib.GenericMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GenericMask &lt;: AbstractArrayMask</code></pre><p>Generic attention mask. Just a wrapper over <code>AbstractArray{Bool}</code> for dispatch.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L64-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.LocalMask" href="#NeuralAttentionlib.LocalMask"><code>NeuralAttentionlib.LocalMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LocalMask(width::Int) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that only allow local (diagonal like) values to pass.</p><p><code>width</code> should be ≥ 0 and <code>A .* LocalMask(1)</code> is similar to <code>Diagonal(A)</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L37-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RandomMask" href="#NeuralAttentionlib.RandomMask"><code>NeuralAttentionlib.RandomMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomMask(p::Float64) &lt;: AbstractDatalessMask</code></pre><p>Attention mask that block value randomly.</p><p><code>p</code> specify the percentage of value to block. e.g. <code>A .* RandomMask(0)</code> is equivalent to <code>identity(A)</code> and  <code>A .* RandomMask(1)</code> is equivalent to <code>zero(A)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L46-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.RepeatMask" href="#NeuralAttentionlib.RepeatMask"><code>NeuralAttentionlib.RepeatMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RepeatMask(mask::AbstractAttenMask, num::Int) &lt;: AbstractWrapperMask</code></pre><p>Attention mask wrapper over array mask for doing inner repeat on the last dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1

julia&gt; trues(3,3, 4) .* m
[...] # wrong result due to out of bound access

julia&gt; trues(3,3, 4) .* RepeatMask(m, 2)
3×3×4 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 3] =
 1  1  1
 1  1  1
 1  1  1

[:, :, 4] =
 1  1  1
 1  1  1
 1  1  1
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L213-L262">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.SymLengthMask" href="#NeuralAttentionlib.SymLengthMask"><code>NeuralAttentionlib.SymLengthMask</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymLengthMask(len::AbstractArray{Int, N}) &lt;: AbstractArrayMask</code></pre><p>Attention mask specified by an array of integer that indicate the length dimension size.  assuming <em>Query</em> length and <em>Key</em> length are the same.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; m = SymLengthMask([2,3])
SymLengthMask{1, Vector{Int32}}(Int32[2, 3])

julia&gt; trues(3,3, 2) .* m
3×3×2 BitArray{3}:
[:, :, 1] =
 1  1  0
 1  1  0
 0  0  0

[:, :, 2] =
 1  1  1
 1  1  1
 1  1  1
</code></pre><p>See also: <a href="#NeuralAttentionlib.BiLengthMask"><code>BiLengthMask</code></a>, <a href="#NeuralAttentionlib.BatchedMask"><code>BatchedMask</code></a>, <a href="#NeuralAttentionlib.RepeatMask"><code>RepeatMask</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L71-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:!-Tuple{NeuralAttentionlib.AbstractAttenMask}" href="#Base.:!-Tuple{NeuralAttentionlib.AbstractAttenMask}"><code>Base.:!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">!m::AbstractAttenMask</code></pre><p>Boolean not of an attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L134-L138">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:&amp;-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}" href="#Base.:&amp;-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}"><code>Base.:&amp;</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractAttenMask &amp; m2::AbstractAttenMask</code></pre><p>logical and of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L149-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:|-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}" href="#Base.:|-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}"><code>Base.:|</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">m1::AbstractAttenMask | m2::AbstractAttenMask</code></pre><p>logical or of two attention mask</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/module/mask.jl#L141-L145">source</a></section></article><h2 id="Matmul"><a class="docs-heading-anchor" href="#Matmul">Matmul</a><a id="Matmul-1"></a><a class="docs-heading-anchor-permalink" href="#Matmul" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.CollapsedDimArray" href="#NeuralAttentionlib.CollapsedDimArray"><code>NeuralAttentionlib.CollapsedDimArray</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CollapsedDimArray{T}(array, si::Integer=2, sj::Integer=3) &lt;: AbstractArray{T, 3}</code></pre><p>Similar to lazy reshape array with <a href="#NeuralAttentionlib.collapsed_size-Tuple{Any, Any, Any}"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/matmul/collapseddim.jl#L83-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.collapsed_size-Tuple{Any, Any, Any}" href="#NeuralAttentionlib.collapsed_size-Tuple{Any, Any, Any}"><code>NeuralAttentionlib.collapsed_size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">collapsed_size(x, xi, xj)::Dim{3}</code></pre><p>Collapse the dimensionality of <code>x</code> into 3 according to <code>xi</code> and <code>xj</code>.</p><pre><code class="nohighlight hljs">(X1, X2, ..., Xi-1, Xi, Xi+1, ..., Xj-1, Xj, ..., Xn)
 |_____dim1______|  |_______dim2______|  |___dim3__|</code></pre><p>This is equivalent to <code>size(reshape(x, prod(size(x)[1:(xi-1)]), prod(size(x)[xi:(xj-1)]), prod(size(x)[xj:end])))</code>.</p><p>#Example</p><pre><code class="language-julia hljs">julia&gt; x = randn(7,6,5,4,3,2);

julia&gt; collapsed_size(x, 3, 5)
(42, 20, 6)
</code></pre><p>See also: <a href="#NeuralAttentionlib.noncollapsed_size-NTuple{4, Any}"><code>noncollapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/matmul/collapseddim.jl#L53-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralAttentionlib.noncollapsed_size-NTuple{4, Any}" href="#NeuralAttentionlib.noncollapsed_size-NTuple{4, Any}"><code>NeuralAttentionlib.noncollapsed_size</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">noncollapsed_size(x, xi, xj, n)</code></pre><p>Collapse the dimensionality of <code>x</code> into 3 according to <code>xi</code> and <code>xj</code>.</p><pre><code class="nohighlight hljs">(X1, X2, ..., Xi-1, Xi, Xi+1, ..., Xj-1, Xj, ..., Xn)
 |_____dim1______|  |_______dim2______|  |___dim3__|</code></pre><p>But take the size before collapse. e.g. <code>noncollapsed_size(x, xi, xj, 2)</code> will be <code>(Xi, Xi+1, ..., Xj-1)</code>.</p><p>#Example</p><pre><code class="language-julia hljs">julia&gt; x = randn(7,6,5,4,3,2);

julia&gt; noncollapsed_size(x, 3, 5, 1)
(7, 6)

julia&gt; noncollapsed_size(x, 3, 5, 2)
(5, 4)

julia&gt; noncollapsed_size(x, 3, 5, 3)
(3, 2)
</code></pre><p>See also: <a href="#NeuralAttentionlib.collapsed_size-Tuple{Any, Any, Any}"><code>collapsed_size</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/181d5d46e74019f979a635b46542b11eb90f606d/src/matmul/collapseddim.jl#L1-L28">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../term/">« Terminology</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.5 on <span class="colophon-date" title="Friday 13 August 2021 11:20">Friday 13 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
