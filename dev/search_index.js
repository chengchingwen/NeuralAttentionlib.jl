var documenterSearchIndex = {"docs":
[{"location":"term/#Terminology","page":"Terminology","title":"Terminology","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"Term and Naming explanation.","category":"page"},{"location":"term/#Prerequisite","page":"Terminology","title":"Prerequisite","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"Some term for better understanding this docs.","category":"page"},{"location":"term/#.-[PartialFunctions](https://github.com/archermarx/PartialFunctions.jl)","page":"Terminology","title":"1. PartialFunctions","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"This actually live outside the scope of this package, but is extremely useful for illustrate the overall design.  We'll use the $ operation to denote partial function application   (i.e. f $ x is equivanlent to (arg...)->f(x, arg...)).","category":"page"},{"location":"term/#.-Feature-/-Length-/-Batch-Dimension","page":"Terminology","title":"2. Feature / Length / Batch Dimension","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"Under the context of attention operation in deep learning, the input data can be viewed as a 3-dimensional array.  The feature dimension, the length dimension, and the batch dimension (f-dim, l-dim, b-dim for short).  Following the Julia's multidimensional array implementation  (column-major),  the data is store in a AbstractArray{T, 3} whose size is (f-dim, l-dim, b-dim).","category":"page"},{"location":"term/","page":"Terminology","title":"Terminology","text":"For example, given 3 sentence as a batch, each sentence have 10 word, and we choose to represent a word with  a vector of 32 element. This data will be store in an 3-dim array with size (32, 10, 3).","category":"page"},{"location":"term/","page":"Terminology","title":"Terminology","text":"General speaking, batch stands for how many independent data you are going to run in one function call,  usually just for performance/optimization need. length means how many entry you have for each data sample,  like the #-words in a sentence or #-pixels in an image. feature is the number of value you used to  represent an entry.","category":"page"},{"location":"term/#Attention","page":"Terminology","title":"Attention","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"The overall attention operation can be viewed as three mutually inclusive block:","category":"page"},{"location":"term/","page":"Terminology","title":"Terminology","text":"\t     (main input)\n\t        Value           Key             Query  (Extras...)\n\t+---------|--------------|----------------|------|||---- Attention Operation ---+\n\t|         |              |                |      |||                            |\n\t|         |              |                |      |||   multihead, ...           |\n\t|         |              |                |      |||                            |\n\t|   +-----|--------------|----------------|------|||-----------------------+    |\n\t|   |     |              |                |      |||                       |    |\n\t|   |     |          +---|----------------|------|||-------------+         |    |\n\t|   |     |          |   |                |      |||             |         |    |\n\t|   |     |          |   |  scoring func  |      |||             |         |    |\n\t|   |     |          |   +------>+<-------+<=======+             |         |    |\n\t|   |     |          |           |                               |         |    |\n\t|   |     |          |           | masked_score,                 |         |    |\n\t|   |     |          |           | normalized_score,             |         |    |\n\t|   |     |          |           | ...                           |         |    |\n\t|   |     |          |           |                               |         |    |\n\t|   |     |          +-----------|------------ Attention Score --+         |    |\n\t|   |     |                      |                                         |    |\n\t|   |     |     mixing func      |                                         |    |\n\t|   |     +--------->+<----------+                                         |    |\n\t|   |                |                                                     |    |\n\t|   +----------------|------------------------------- Mixing --------------+    |\n\t|                    |                                                          |\n\t+--------------------|----------------------------------------------------------+\n\t              Attentive Value\n\t               (main output)","category":"page"},{"location":"term/","page":"Terminology","title":"Terminology","text":"The attention operation is actually a special way to \"mix\" (or \"pick\" in common lecture) the input information.  In (probably) the first attention paper, the attention is defined as weighted  sum of the input sequence given a word embedding. The idea is furthur generalize to QKV attention in the first  transformer paper. ","category":"page"},{"location":"term/#Attention-Score","page":"Terminology","title":"Attention Score","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"The attention score is used to decide how much the each piece of input information will contribute to the  output value and also how many entry the attention operation will output. The operation that will modify  the attention score matrix should be consider as part of this block. For example: Different attention masks  (local attention, random attention, ...), normalization (softmax, l2-norm, ...), and some special attention  that take other inputs (transformer decoder, relative position encoding, ...).","category":"page"},{"location":"term/#Mixing","page":"Terminology","title":"Mixing","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"We refer to the operation that take the attention score and input value as \"mixing\". Usually it's just a  weighted sum over the input value and use the attention score as the weight.","category":"page"},{"location":"term/#Attention-Operation","page":"Terminology","title":"Attention Operation","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"The whole scoring + mixing and other pre/post processing made up an attention operation. Things like handling  multi-head should happen at this level.","category":"page"},{"location":"term/#Attention-Mask","page":"Terminology","title":"Attention Mask","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"Attention masks are a bunch of operation that modified the attention score.","category":"page"},{"location":"term/#Dataless-mask","page":"Terminology","title":"Dataless mask","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"We use \"dataless\" to refer to masks that are independent to the input. For example, CausalMask works the same  on each data regardless of the batch size or the data content.","category":"page"},{"location":"term/#Array-mask","page":"Terminology","title":"Array mask","text":"","category":"section"},{"location":"term/","page":"Terminology","title":"Terminology","text":"We call the mask that is dependent to the input as \"array mask\". For example, SymLengthMask is used to avoid  the padding token being considered in the attention operation, thus each data batch might have different mask value.","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Order   = [:function, :type]","category":"page"},{"location":"api/#Mask","page":"API Reference","title":"Mask","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [NeuralAttentionlib, NeuralAttentionlib.Masks]\nPages   = [\"mask.jl\"]","category":"page"},{"location":"api/#NeuralAttentionlib.AbstractAttenMask","page":"API Reference","title":"NeuralAttentionlib.AbstractAttenMask","text":"AbstractAttenMask\n\nAbstract type for mask data, can be viewed as AbstractArray{Bool}\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.AbstractAttenMaskOp","page":"API Reference","title":"NeuralAttentionlib.AbstractAttenMaskOp","text":"AbstractAttenMaskOp\n\nTrait-like abstract type for holding operation related argument, defined how the mask should be apply to input array\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}","page":"API Reference","title":"NeuralAttentionlib.apply_mask","text":"apply_mask(op::GenericAttenMaskOp, mask::AbstractAttenMask, score)\n\nEquivalent to op.apply(score, op.scale .* (op.flip ? .! mask : mask)).\n\nExample\n\njulia> x = randn(10, 10);\n\njulia> m = CausalMask()\nCausalMask()\n\njulia> apply_mask(GenericAttenMaskOp(.+, true, -1e9), m, x) ==  @. x + (!m * -1e9)\ntrue\n\n\n\n\n\n\n","category":"method"},{"location":"api/#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}","page":"API Reference","title":"NeuralAttentionlib.apply_mask","text":"apply_mask(op::NaiveAttenMaskOp, mask::AbstractAttenMask, score)\n\nDirectly broadcast multiply mask to attention score, i.e. score .* mask.\n\n\n\n\n\n","category":"method"},{"location":"api/#NeuralAttentionlib.AbstractArrayMask","page":"API Reference","title":"NeuralAttentionlib.AbstractArrayMask","text":"AbstractArrayMask <: AbstractAttenMask\n\nAbstract type for mask with array data\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.AbstractDatalessMask","page":"API Reference","title":"NeuralAttentionlib.AbstractDatalessMask","text":"AbstractDatalessMask <: AbstractAttenMask\n\nAbstract type for mask without array data.\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.BandPartMask","page":"API Reference","title":"NeuralAttentionlib.BandPartMask","text":"BandPartMask(l::Int, u::Int) <: AbstractDatalessMask\n\nAttention mask that only allow band_part  values to pass.\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.BatchedMask","page":"API Reference","title":"NeuralAttentionlib.BatchedMask","text":"BatchedMask(mask::AbstractArrayMask, batch_dim::Int) <: AbstractWrapperMask\n\nAttention mask wrapper over array mask for applying the same mask within the same batch.  Use batch_dim to specify from which dim should be treated as batch dims.\n\nExample\n\njulia> m = SymLengthMask([2,3])\nSymLengthMask{1, Vector{Int32}}(Int32[2, 3])\n\njulia> trues(3,3, 2) .* m\n3×3×2 BitArray{3}:\n[:, :, 1] =\n 1  1  0\n 1  1  0\n 0  0  0\n\n[:, :, 2] =\n 1  1  1\n 1  1  1\n 1  1  1\n\njulia> trues(3,3, 2, 2) .* m\nERROR: [...]\n\njulia> trues(3,3, 2, 2) .* BatchedMask(m, 4) # 4-th dim is batch dim\n3×3×2×2 BitArray{4}:\n[:, :, 1, 1] =\n 1  1  0\n 1  1  0\n 0  0  0\n\n[:, :, 2, 1] =\n 1  1  0\n 1  1  0\n 0  0  0\n\n[:, :, 1, 2] =\n 1  1  1\n 1  1  1\n 1  1  1\n\n[:, :, 2, 2] =\n 1  1  1\n 1  1  1\n 1  1  1\n\n# can also use negative value to count from the last\njulia> trues(3,3, 2, 2) .* BatchedMask(m, 4) == trues(3,3, 2, 2) .* BatchedMask(m, -1)\ntrue\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.BiLengthMask","page":"API Reference","title":"NeuralAttentionlib.BiLengthMask","text":"BiLengthMask(q_len::A, k_len::A) where {A <: AbstractArray{Int, N}} <: AbstractArrayMask\n\nAttention mask specified by two arrays of integer that indicate the length dimension size.\n\nExample\n\njulia> bm = BiLengthMask([2,3], [3, 5])\nBiLengthMask{1, Vector{Int32}}(Int32[2, 3], Int32[3, 5])\n\njulia> trues(5,5, 2) .* bm\n5×5×2 BitArray{3}:\n[:, :, 1] =\n 1  1  0  0  0\n 1  1  0  0  0\n 1  1  0  0  0\n 0  0  0  0  0\n 0  0  0  0  0\n\n[:, :, 2] =\n 1  1  1  0  0\n 1  1  1  0  0\n 1  1  1  0  0\n 1  1  1  0  0\n 1  1  1  0  0\n\n\nSee also: SymLengthMask, BatchedMask, RepeatMask\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.CausalMask","page":"API Reference","title":"NeuralAttentionlib.CausalMask","text":"CausalMask() <: AbstractDatalessMask\n\nAttention mask that block the future values.\n\nSimilar to applying LinearAlgebra.triu! on the score matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.GenericMask","page":"API Reference","title":"NeuralAttentionlib.GenericMask","text":"GenericMask <: AbstractArrayMask\n\nGeneric attention mask. Just a wrapper over AbstractArray{Bool} for dispatch.\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.LocalMask","page":"API Reference","title":"NeuralAttentionlib.LocalMask","text":"LocalMask(width::Int) <: AbstractDatalessMask\n\nAttention mask that only allow local (diagonal like) values to pass.\n\nwidth should be ≥ 0 and A .* LocalMask(1) is similar to Diagonal(A)\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.RandomMask","page":"API Reference","title":"NeuralAttentionlib.RandomMask","text":"RandomMask(p::Float64) <: AbstractDatalessMask\n\nAttention mask that block value randomly.\n\np specify the percentage of value to block. e.g. A .* RandomMask(0) is equivalent to identity(A) and  A .* RandomMask(1) is equivalent to zero(A).\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.RepeatMask","page":"API Reference","title":"NeuralAttentionlib.RepeatMask","text":"RepeatMask(mask::AbstractAttenMask, num::Int) <: AbstractWrapperMask\n\nAttention mask wrapper over array mask for doing inner repeat on the last dimension.\n\nExample\n\njulia> m = SymLengthMask([2,3])\nSymLengthMask{1, Vector{Int32}}(Int32[2, 3])\n\njulia> trues(3,3, 2) .* m\n3×3×2 BitArray{3}:\n[:, :, 1] =\n 1  1  0\n 1  1  0\n 0  0  0\n\n[:, :, 2] =\n 1  1  1\n 1  1  1\n 1  1  1\n\njulia> trues(3,3, 4) .* m\n[...] # wrong result due to out of bound access\n\njulia> trues(3,3, 4) .* RepeatMask(m, 2)\n3×3×4 BitArray{3}:\n[:, :, 1] =\n 1  1  0\n 1  1  0\n 0  0  0\n\n[:, :, 2] =\n 1  1  0\n 1  1  0\n 0  0  0\n\n[:, :, 3] =\n 1  1  1\n 1  1  1\n 1  1  1\n\n[:, :, 4] =\n 1  1  1\n 1  1  1\n 1  1  1\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.SymLengthMask","page":"API Reference","title":"NeuralAttentionlib.SymLengthMask","text":"SymLengthMask(len::AbstractArray{Int, N}) <: AbstractArrayMask\n\nAttention mask specified by an array of integer that indicate the length dimension size.  assuming Query length and Key length are the same.\n\nExample\n\njulia> m = SymLengthMask([2,3])\nSymLengthMask{1, Vector{Int32}}(Int32[2, 3])\n\njulia> trues(3,3, 2) .* m\n3×3×2 BitArray{3}:\n[:, :, 1] =\n 1  1  0\n 1  1  0\n 0  0  0\n\n[:, :, 2] =\n 1  1  1\n 1  1  1\n 1  1  1\n\n\nSee also: BiLengthMask, BatchedMask, RepeatMask\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.:!-Tuple{NeuralAttentionlib.AbstractAttenMask}","page":"API Reference","title":"Base.:!","text":"!m::AbstractAttenMask\n\nBoolean not of an attention mask\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.:&-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}","page":"API Reference","title":"Base.:&","text":"m1::AbstractAttenMask & m2::AbstractAttenMask\n\nlogical and of two attention mask\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.:|-Tuple{NeuralAttentionlib.AbstractAttenMask, NeuralAttentionlib.AbstractAttenMask}","page":"API Reference","title":"Base.:|","text":"m1::AbstractAttenMask | m2::AbstractAttenMask\n\nlogical or of two attention mask\n\n\n\n\n\n","category":"method"},{"location":"api/#Matmul","page":"API Reference","title":"Matmul","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [NeuralAttentionlib]\nPages   = [\"collapseddim.jl\", \"matmul.jl\"]","category":"page"},{"location":"api/#NeuralAttentionlib.CollapsedDimArray","page":"API Reference","title":"NeuralAttentionlib.CollapsedDimArray","text":"CollapsedDimArray{T}(array, si::Integer=2, sj::Integer=3) <: AbstractArray{T, 3}\n\nSimilar to lazy reshape array with collapsed_size\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.collapsed_size-Tuple{Any, Any, Any}","page":"API Reference","title":"NeuralAttentionlib.collapsed_size","text":"collapsed_size(x, xi, xj)::Dim{3}\n\nCollapse the dimensionality of x into 3 according to xi and xj.\n\n(X1, X2, ..., Xi-1, Xi, Xi+1, ..., Xj-1, Xj, ..., Xn)\n |_____dim1______|  |_______dim2______|  |___dim3__|\n\nThis is equivalent to size(reshape(x, prod(size(x)[1:(xi-1)]), prod(size(x)[xi:(xj-1)]), prod(size(x)[xj:end]))).\n\n#Example\n\njulia> x = randn(7,6,5,4,3,2);\n\njulia> collapsed_size(x, 3, 5)\n(42, 20, 6)\n\n\nSee also: noncollapsed_size\n\n\n\n\n\n","category":"method"},{"location":"api/#NeuralAttentionlib.noncollapsed_size-NTuple{4, Any}","page":"API Reference","title":"NeuralAttentionlib.noncollapsed_size","text":"noncollapsed_size(x, xi, xj, n)\n\nCollapse the dimensionality of x into 3 according to xi and xj.\n\n(X1, X2, ..., Xi-1, Xi, Xi+1, ..., Xj-1, Xj, ..., Xn)\n |_____dim1______|  |_______dim2______|  |___dim3__|\n\nBut take the size before collapse. e.g. noncollapsed_size(x, xi, xj, 2) will be (Xi, Xi+1, ..., Xj-1).\n\n#Example\n\njulia> x = randn(7,6,5,4,3,2);\n\njulia> noncollapsed_size(x, 3, 5, 1)\n(7, 6)\n\njulia> noncollapsed_size(x, 3, 5, 2)\n(5, 4)\n\njulia> noncollapsed_size(x, 3, 5, 3)\n(3, 2)\n\n\nSee also: collapsed_size\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = NeuralAttentionlib","category":"page"},{"location":"#[NeuralAttentionlib](https://github.com/chengchingwen/NeuralAttentionlib.jl)","page":"Home","title":"NeuralAttentionlib","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Reusable functionality for defining custom attention/transformer layers.","category":"page"},{"location":"#Outline","page":"Home","title":"Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n\t\"term.md\",\n\t\"api.md\",\n]","category":"page"}]
}
