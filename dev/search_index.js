var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [NeuralAttentionlib]","category":"page"},{"location":"api/#NeuralAttentionlib.AbstractAttenMask","page":"API Reference","title":"NeuralAttentionlib.AbstractAttenMask","text":"Wrapper type for mask data, can be viewed as AbstractArray{Bool}\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.AbstractAttenMaskOp","page":"API Reference","title":"NeuralAttentionlib.AbstractAttenMaskOp","text":"Trait-like type for holding operation related argument, defined how the mask should be apply to input array\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.GenericAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}","page":"API Reference","title":"NeuralAttentionlib.apply_mask","text":"Equivalent to op.apply(score, op.scale .* (op.flip ? .! mask : mask)).\n\nFor example: apply_generic_mask(GenericAttenMaskOp(.+, static(true), -1e9), mask, score) == @. score + (!mask * -1e9).\n\n\n\n\n\n","category":"method"},{"location":"api/#NeuralAttentionlib.apply_mask-Tuple{NeuralAttentionlib.NaiveAttenMaskOp, NeuralAttentionlib.AbstractAttenMask, Any}","page":"API Reference","title":"NeuralAttentionlib.apply_mask","text":"Directly broadcast multiply mask to attention score.\n\n\n\n\n\n","category":"method"},{"location":"api/#NeuralAttentionlib.collapsed_size-Tuple{Any, Any, Any}","page":"API Reference","title":"NeuralAttentionlib.collapsed_size","text":"collapsed_size(x, xi, xj)\n\nCollapse the dimensionality of x into 3 according to xi and xj.\n\n(X1, X2, ..., Xi-1, Xi, Xi+1, ..., Xj-1, Xj, ..., Xn)\n |_____dim1______|  |_______dim2______|  |___dim3__|\n\nThis is equivalent to size(reshape(x, prod(size(x)[1:(xi-1)]), prod(size(x)[xi:(xj-1)]), prod(size(x)[xj:end]))).\n\n#Example\n\njulia> x = randn(7,6,5,4,3,2);\n\njulia> collapsed_size(x, 3,5)\n(42, 20, 6)\n\n\n\n\n\n\n","category":"method"},{"location":"api/#NeuralAttentionlib.naive_qkv_attention","page":"API Reference","title":"NeuralAttentionlib.naive_qkv_attention","text":"equivalent to generic_qkv_attention(weighted_sum_mixing, normalized_score(NNlib.softmax) $ masked_score(mask) $ scaled_dot_product_score, q, k, v)\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = NeuralAttentionlib","category":"page"},{"location":"#[NeuralAttentionlib](https://github.com/chengchingwen/NeuralAttentionlib.jl)","page":"Home","title":"NeuralAttentionlib","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Reusable functionality for defining custom attention/transformer layers.","category":"page"},{"location":"#Outline","page":"Home","title":"Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n\t\"api.md\",\n]","category":"page"}]
}
