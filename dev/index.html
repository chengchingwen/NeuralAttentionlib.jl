<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · NeuralAttentionlib.jl</title><meta name="title" content="Home · NeuralAttentionlib.jl"/><meta property="og:title" content="Home · NeuralAttentionlib.jl"/><meta property="twitter:title" content="Home · NeuralAttentionlib.jl"/><meta name="description" content="Documentation for NeuralAttentionlib.jl."/><meta property="og:description" content="Documentation for NeuralAttentionlib.jl."/><meta property="twitter:description" content="Documentation for NeuralAttentionlib.jl."/><meta property="og:url" content="https://chengchingwen.github.io/NeuralAttentionlib.jl/"/><meta property="twitter:url" content="https://chengchingwen.github.io/NeuralAttentionlib.jl/"/><link rel="canonical" href="https://chengchingwen.github.io/NeuralAttentionlib.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>NeuralAttentionlib.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Design"><span>Design</span></a></li><li class="toplevel"><a class="tocitem" href="#Outline"><span>Outline</span></a></li></ul></li><li><a class="tocitem" href="term/">Terminology</a></li><li><a class="tocitem" href="example/">Example</a></li><li><a class="tocitem" href="api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/chengchingwen/NeuralAttentionlib.jl/blob/master/docs/src/index.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="[NeuralAttentionlib](https://github.com/chengchingwen/NeuralAttentionlib.jl)"><a class="docs-heading-anchor" href="#[NeuralAttentionlib](https://github.com/chengchingwen/NeuralAttentionlib.jl)"><a href="https://github.com/chengchingwen/NeuralAttentionlib.jl">NeuralAttentionlib</a></a><a id="[NeuralAttentionlib](https://github.com/chengchingwen/NeuralAttentionlib.jl)-1"></a><a class="docs-heading-anchor-permalink" href="#[NeuralAttentionlib](https://github.com/chengchingwen/NeuralAttentionlib.jl)" title="Permalink"></a></h1><p><em>Reusable functionality for defining custom attention/transformer layers.</em></p><p><code>NeuralAttentionlib.jl</code> aim to be highly extendable and reusable function for implementing attention variants.  Will be powering <a href="https://github.com/chengchingwen/Transformers.jl"><code>Transformers.jl</code></a>.</p><h1 id="Design"><a class="docs-heading-anchor" href="#Design">Design</a><a id="Design-1"></a><a class="docs-heading-anchor-permalink" href="#Design" title="Permalink"></a></h1><p><img src="assets/overview.png" alt="overview"/></p><p>The core idea of this package is to make the attention operation composable, so that most of the attention variants can  be easily defined without rewriting other parts. For example, normal attention use <code>softmax</code> on the attention score to  normalize weight of each entries. If you want to replace <code>softmax</code> with other normalization function, such as L2-norm,  there is a problem that they require different ways to mask specific entries such as paddings. With this package, we  can easily do this by providing a different <code>AbstractMaskOp</code> to <code>masked_score</code>, so no copy-paste is needed. For another  example, some position embeddings are adding values to the attention scores, with this package, you can directly chain  the position embedding function (or use <code>biased_score</code>) with other score functions. Moreover, the same definition can  be used directly for high dimensional attentions, such as image or video.</p><p>This package contain 3 submodules: <code>Matmul</code>, <code>Masks</code>, and <code>Functional</code>.</p><ol><li><code>Matmul</code> defines an Array wrapper <code>CollapsedDimsArray{T}(array, ni::Integer, nj::Integer)</code> which treat n-dimensional array as 3-dimensional array while preserving the original shape. By explicitly specifying which dimensions should be the &quot;batch&quot; and &quot;length&quot; dimensions, the implementations of attention do not need to worry about the input dimensions.</li><li><code>Masks</code> provides an interface to define non-allocating masks with support for both CPU and GPU (using Julia&#39;s broadcast interface) and many pre-defined masks. For example, <code>CausalMask()</code> is just a Julia object and it would NOT allocate a <code>n^2</code> attention score mask either on CPU or GPU. These masks are also composable, you can use <code>&amp;</code>/<code>|</code> to combine, for example, causal mask and padding mask without extra allocation or the need to write extra code.</li><li><code>Functional</code> contains the implementation for the &quot;attention score&quot;s, &quot;mixing&quot;s, and &quot;attention operation&quot;s. The interface of &quot;attention score&quot;s allow you to chain different score function together, such as <code>normalized_score</code>, <code>masked_score</code>, and <code>biased_score</code>. And the interface of &quot;attention operation&quot;s allow you to provide different score functions and mixing functions. The other part, such as reshaping for multi-head, are automatically handled.</li></ol><h1 id="Outline"><a class="docs-heading-anchor" href="#Outline">Outline</a><a id="Outline-1"></a><a class="docs-heading-anchor-permalink" href="#Outline" title="Permalink"></a></h1><ul><li><a href="term/#Terminology">Terminology</a></li><li class="no-marker"><ul><li><a href="term/#Prerequisite">Prerequisite</a></li><li><a href="term/#Attention">Attention</a></li><li><a href="term/#Attention-Mask">Attention Mask</a></li></ul></li><li><a href="example/#Example">Example</a></li><li class="no-marker"><ul><li><a href="example/#Comparing-to-the-existing-implementation-in-Transformers.jl">Comparing to the existing implementation in Transformers.jl</a></li></ul></li><li><a href="api/#API-Reference">API Reference</a></li><li class="no-marker"><ul><li><a href="api/#Functional">Functional</a></li><li><a href="api/#Mask">Mask</a></li><li><a href="api/#Matmul">Matmul</a></li></ul></li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="term/">Terminology »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Tuesday 4 June 2024 14:14">Tuesday 4 June 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
